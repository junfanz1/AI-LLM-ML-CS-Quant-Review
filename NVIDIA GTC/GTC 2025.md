# GTC 2025 Overview & Takeaways

Junfan Zhu ğŸ‘‹ 2025-03-23

<div align="left">
  <marquee behavior="alternate" scrollamount="3">
    <strong>Profile Views:</strong>
    <img src="https://komarev.com/ghpvc/?username=junfanz1&color=blue" alt="Profile Views" />
    &nbsp;â€¢&nbsp;
    <strong>Followers:</strong>
    <img src="https://img.shields.io/github/followers/junfanz1?style=social" alt="GitHub Followers" />
    &nbsp;â€¢&nbsp;
    <strong>Stars:</strong>
    <img src="https://img.shields.io/github/stars/junfanz1?style=social" alt="GitHub Stars" />
  </marquee>
</div>

<div align="left">
  <marquee behavior="alternate" scrollamount="3">
    <strong>Connect with me:</strong>
    &nbsp;
    <a href="https://github.com/junfanz1">GitHub</a> â€¢
    <a href="https://www.overleaf.com/read/jcgfkzhyfvdv#57139d">Resume</a> â€¢
    <a href="https://www.linkedin.com/in/junfan-zhu/">LinkedIn</a> â€¢
    <a href="https://x.com/junfanzhu98">X</a> â€¢
    <a href="mailto:junfanzhu98@gmail.com">Email</a> â€¢
    <a href="https://www.instagram.com/junfan_zhu/">Instagram</a> â€¢
    <a href="https://www.facebook.com/junfan.zhu.961/">Facebook</a> â€¢
    <a href="https://www.douban.com/people/junfanz/">Douban</a> â€¢
    <a href="junfanzhu98">WeChat</a>
  </marquee>
</div>

How much of the AI revolution did you catch at [GTC Conference 2025](https://www.nvidia.com/gtc/)?

2025 GTC was an enriching experience which deepened my insights into cutting-edge AI research and transformative technology trends - from multimodal architecture breakthroughs to LLM optimization techniques. Iâ€™ve distilled my key takeaways into detailed notes on [GitHub](https://github.com/junfanz1/AI-LLM-ML-CS-Quant-Overview/blob/main/NVIDIA%20GTC/GTC%202025.md), highlighting selected talks that captured my interest and shaped my perspective on next-gen AI systems.

As an AI researcher specializing in LLM systems and optimization, I'm energized to implement these insights while actively exploring roles pushing boundaries in:
- Scalable model architectures and pipelines design
- Novel approaches to Multimodal agentic reasoning-optimized systems

Let's dissect these developments together! Welcoming technical discussions, collaborative deep dives, and knowledge-sharing with fellow practitioners. The repository remains actively updated - contributions and extensions are warmly encouraged. If you find it useful, feel free to give me a Github star. 

<div style="display: inline-block; border: 1px solid lightgray; padding: 0;">
  <table style="border-collapse: collapse; transform: scale(0.5); transform-origin: 0 0;">
    <tr>
      <td style="padding: 0;">
        <img src="https://github.com/user-attachments/assets/17a05ffb-200b-437e-9b66-cd003abc6c8e" 
             style="display: block; width: 300px; height: auto;">
      </td>
    </tr>
    <tr>
      <td style="padding: 2px 0 0 0;">
        <em style="display: block; font-size: 12px; line-height: 1; margin: 0;">Project integrating Generative AI, Humanoid Robotics (RLHF), and Low-Altitude Economy.</em>
      </td>
    </tr>
  </table>
</div>


## Contents

<!-- TOC start (generated with https://github.com/derlin/bitdowntoc) -->

- [Summary](#summary)
   * [TLDR Summary](#tldr-summary)
   * [Technical Focus Areas](#technical-focus-areas)
- [Chinese: 1-100](#chinese-1-100)
   * [1. AIâ€¯åˆ›ä¸šä¼ä¸šåœ¨ä¸­å›½çš„å‘å±•ä¸åŠ©åŠ› [S73846]](#1-ai-s73846)
   * [2. The embodied end-to-end VLA model driven by synthetic big data åˆæˆå¤§æ•°æ®é©±åŠ¨çš„å…·èº«ç«¯åˆ°ç«¯ VLA å¤§æ¨¡å‹ [S71942]](#2-the-embodied-end-to-end-vla-model-driven-by-synthetic-big-data-vla-s71942)
   * [3. Mobile-Agent: æ¢ç´¢åŸºäºå¤šæ¨¡æ€æ™ºèƒ½ä½“çš„æ±½è½¦åº§èˆ±åŠ©æ‰‹æ–°æŠ€æœ¯ [S72561]](#3-mobile-agent-s72561)
   * [4. åˆ›ä¸šä¼ä¸šåœ¨ç”Ÿæˆå¼ AI åŠæœºå™¨äººæ–¹å‘çš„å®è·µä¸åˆ†äº« [S73910]](#4-ai-s73910)
   * [5. åº”ç”¨äºæ±½è½¦è¡Œä¸šèŠå¤©æœºå™¨äººçš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ¡ˆ [S72500]](#5-s72500)
   * [6. å¤§è¯­è¨€æ¨¡å‹åœ¨æ™ºèƒ½åº§èˆ±ä¸­çš„åº”ç”¨ [S72716]](#6-s72716)
   * [7. æ¿€å‘é€šç”¨äººå·¥æ™ºèƒ½çš„åˆ›é€ åŠ›ï¼Œå¼•é¢†æ™ºèƒ½æ±½è½¦èµ°å‘æ–°çš„æœªæ¥ [S72635]](#7-s72635)
   * [8. åŠ é€ŸæŒ‡æ ‡è®¡ç®—ï¼šCPU/GPU å¼‚æ„å®æ—¶è®¡ç®—å¹³å° [S71579]](#8-cpugpu-s71579)
   * [9. UFO-Lite: åŸºäºè‡ªæ¨æµ‹è§£ç çš„ä½å»¶è¿Ÿå¤šæ¨¡æ€å¤§æ¨¡å‹ [S72498]](#9-ufo-lite-s72498)
   * [10. VLAï¼šè¿ˆå‘è‡ªåŠ¨é©¾é©¶ç‰©ç†æ™ºèƒ½ä½“çš„å…³é”®ä¸€æ­¥ [S72557]](#10-vla-s72557)
   * [11. ä½¿ç”¨æŠ•æœºé‡‡æ ·å’Œè®¡ç®—é€šä¿¡ Overlap æå‡ LLM æ¨ç†æ•ˆç‡ [S72643]](#11-overlap-llm-s72643)
   * [12. æ„å»ºä»¥ Megatron-Core ä¸ºæ ¸å¿ƒçš„å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒåŠ é€Ÿç”Ÿæ€ [S72580]](#12-megatron-core-s72580)
   * [13. é‡å¡‘çŸ­è§†é¢‘è§†è§‰ä½“éªŒï¼ŒåŸºäº TensorRT-LLM åŠ é€Ÿçš„æ™ºèƒ½è§†é¢‘è´¨é‡è¯„ä»·ä¸å¤„ç†å¤§æ¨¡å‹ [S74181]](#13-tensorrt-llm-s74181)
   * [14. Laiye AI Foundry - NVIDIA AI Enterprise åœ¨ä¸­å›½çš„æœ€ä½³å®è·µ [S72276]](#14-laiye-ai-foundry-nvidia-ai-enterprise-s72276)
   * [15. LLM 2-bit åé‡åŒ–çš„åŠ é€Ÿä¸éƒ¨ç½²å®è·µ [S72647]](#15-llm-2-bit-s72647)
   * [16. é¢å‘æµ·é‡æ¨¡å‹ä¸šåŠ¡åœºæ™¯çš„æ–‡ç”Ÿå›¾é«˜æ•ˆæ¨ç†åŠ é€Ÿè§£å†³æ–¹æ¡ˆ [S72639]](#16-s72639)
   * [17. ä¸‹ä¸€ä»£ç”Ÿæˆå¼æ¨èæ¨¡å‹è®­æ¨å¼•æ“çš„å»ºè®¾å’Œè½åœ°å®è·µ [S74073]](#17-s74073)
   * [18. åŸºäº TensorRT-LLM çš„å¹¿å‘Šåœºæ™¯ç”Ÿæˆå¼æ¨ç†åŠ é€Ÿæ–¹æ¡ˆ [S72995]](#18-tensorrt-llm-s72995)
- [English: 101-200](#english-101-200)
   * [101. Enable Intelligent Storage to Process Data for AI Applications [S71937]](#101-enable-intelligent-storage-to-process-data-for-ai-applications-s71937)
   * [102. Mistral AI: Placing Frontier AI in Your Hands [S73942]](#102-mistral-ai-placing-frontier-ai-in-your-hands-s73942)
   * [103. How to Ace a Finance Developer Interview: A Deep Dive into GPU Matrix Optimization [S73619]](#103-how-to-ace-a-finance-developer-interview-a-deep-dive-into-gpu-matrix-optimization-s73619)
   * [104. GTC 2025 Keynote [S72484]](#104-gtc-2025-keynote-s72484)
   * [105. AI for Safe and Efficient Trading in Electronic Markets [S72692]](#105-ai-for-safe-and-efficient-trading-in-electronic-markets-s72692)
   * [106. NVIDIA Nventures Showcase: AI Agents in Physical and Virtual Worlds [DD73694]](#106-nvidia-nventures-showcase-ai-agents-in-physical-and-virtual-worlds-dd73694)
   * [107. Google AI: Research, Progress, and the Future of Ads [S73303]](#107-google-ai-research-progress-and-the-future-of-ads-s73303)
   * [108. Frontiers of AI and Computing: A Conversation With Yann LeCun and Bill Dally [S73208]](#108-frontiers-of-ai-and-computing-a-conversation-with-yann-lecun-and-bill-dally-s73208)
   * [109. Building a Scalable Enterprise Multi-Agent Platform for Financial Services [S72854]](#109-building-a-scalable-enterprise-multi-agent-platform-for-financial-services-s72854)
   * [110. Distributed Agentic Multi-Modal LLM Deployments [S72654]](#110-distributed-agentic-multi-modal-llm-deployments-s72654)
   * [111. AI Agents in Production: Insights and Future Directions [S72884]](#111-ai-agents-in-production-insights-and-future-directions-s72884)
   * [112. Insights From NVIDIA Research [S73202]](#112-insights-from-nvidia-research-s73202)
      + [GPU performance bottlenecks for LLM workloads with Optimizations for Memory](#gpu-performance-bottlenecks-for-llm-workloads-with-optimizations-for-memory)
      + [Hybrid-head LLMs and Hymba Architecture](#hybrid-head-llms-and-hymba-architecture)
   * [113. Scaling Vision LLMs to extract and deploy targeted metadata for Catalog Management [S73701]](#113-scaling-vision-llms-to-extract-and-deploy-targeted-metadata-for-catalog-management-s73701)
   * [114. Productionize LLMs for Quantitative Analysis of Market Risk: An Exploratory Attempt [S73818]](#114-productionize-llms-for-quantitative-analysis-of-market-risk-an-exploratory-attempt-s73818)
   * [115. An Introduction to Building Humanoid Robots [S72590]](#115-an-introduction-to-building-humanoid-robots-s72590)
   * [116. Leveraging Large Model-Based Embodied Intelligence to Enhance Financial Service Robots [S71267]](#116-leveraging-large-model-based-embodied-intelligence-to-enhance-financial-service-robots-s71267)
   * [117. AI for Humanoid Robots [S73182]](#117-ai-for-humanoid-robots-s73182)
   * [118. Jane Street: How an Early AI Adopter Thinks About Infrastructure (Presented by CoreWeave) [S74219]](#118-jane-street-how-an-early-ai-adopter-thinks-about-infrastructure-presented-by-coreweave-s74219)
   * [119. The Promise of Humanoid Robots: Research vs. the Real World [S72592]](#119-the-promise-of-humanoid-robots-research-vs-the-real-world-s72592)
   * [120. A New Era of Generalist Robotics: The Rise of Humanoids [S72543]](#120-a-new-era-of-generalist-robotics-the-rise-of-humanoids-s72543)
   * [121. Tencent HunYuan: Building a High-Performance Inference Engine for Large Models Based on NVIDIA TensorRT-LLM [S71563]](#121-tencent-hunyuan-building-a-high-performance-inference-engine-for-large-models-based-on-nvidia-tensorrt-llm-s71563)
   * [122. Accelerate Super Long-Context LLM Inference [S72568]](#122-accelerate-super-long-context-llm-inference-s72568)
   * [123. Scaling AI Platform at LinkedIn: LLMs, Agents, GPUs, Kernels, and More [S72963]](#123-scaling-ai-platform-at-linkedin-llms-agents-gpus-kernels-and-more-s72963)
   * [124. Streamlining Investment Insights for Wealth Management with Generative AI [S71653]](#124-streamlining-investment-insights-for-wealth-management-with-generative-ai-s71653)
   * [125. Unlocking the Future of LLMs: High-Efficiency xLSTM Architectures for Scalable Performance [S71812]](#125-unlocking-the-future-of-llms-high-efficiency-xlstm-architectures-for-scalable-performance-s71812)
   * [126. The Future of AI: Scaling Intelligence, Open-Source Innovation, and Human-AI Collaboration [S73863]](#126-the-future-of-ai-scaling-intelligence-open-source-innovation-and-human-ai-collaboration-s73863)
   * [127. Announcing Mujoco-Warp and Newton: How Google DeepMind and NVIDIA are Supercharging Robotics Development [S72709]](#127-announcing-mujoco-warp-and-newton-how-google-deepmind-and-nvidia-are-supercharging-robotics-development-s72709)
   * [128. Unlocking High-Performance AI Applications at Airbnb [S73265]](#128-unlocking-high-performance-ai-applications-at-airbnb-s73265)
   * [129. FlashAttention-3: Fast and Accurate Attention With Asynchrony and Low Precision [S71368]](#129-flashattention-3-fast-and-accurate-attention-with-asynchrony-and-low-precision-s71368)
   * [130. Scaling and Leveraging Multi-Modal Large Models for Roblox's Metaverse [S72911]](#130-scaling-and-leveraging-multi-modal-large-models-for-robloxs-metaverse-s72911)
   * [131. Quantum Computing: Where We Are and Where Weâ€™re Headed [S74495]](#131-quantum-computing-where-we-are-and-where-were-headed-s74495)
   * [132. Peering Into the Future: What AI and Graph Networks Can Mean for the Future of Financial Analysis [S74726]](#132-peering-into-the-future-what-ai-and-graph-networks-can-mean-for-the-future-of-financial-analysis-s74726)
   * [133. Horizontal Scaling of LLM Training with JAX [S73266]](#133-horizontal-scaling-of-llm-training-with-jax-s73266)
   * [134. Toward Multidisciplinary Scientific Foundation Models [S72256]](#134-toward-multidisciplinary-scientific-foundation-models-s72256)
   * [135. Kolmogorov AI Networks for Interpretable Financial Forecasting [S72542]](#135-kolmogorov-ai-networks-for-interpretable-financial-forecasting-s72542)
   * [136. Scaling Distributed AI Applications with Ray [S71766]](#136-scaling-distributed-ai-applications-with-ray-s71766)
   * [137. Startup Pitches: Generative AI, LLMs, and Data [S74529]](#137-startup-pitches-generative-ai-llms-and-data-s74529)
   * [138. Integrate LLMs Into Asset Management to Drive Innovation and Efficiency [S71575]](#138-integrate-llms-into-asset-management-to-drive-innovation-and-efficiency-s71575)
   * [139. From RAG to Agents: Building Enterprise Products with Generative AI [S71685]](#139-from-rag-to-agents-building-enterprise-products-with-generative-ai-s71685)
   * [140. Advanced RAG Pipelines: Engineer Scalable Retrieval Systems for Enterprise AI [S73015]](#140-advanced-rag-pipelines-engineer-scalable-retrieval-systems-for-enterprise-ai-s73015)
   * [141. Enhance LLMs With Writing in the Margins (WiM): A Better Inference Pattern for Long-Context Retrieval [S72323]](#141-enhance-llms-with-writing-in-the-margins-wim-a-better-inference-pattern-for-long-context-retrieval-s72323)

<!-- TOC end -->

---

<!-- TOC --><a name="summary"></a>
# Summary

<!-- TOC --><a name="tldr-summary"></a>
## TLDR Summary

AI Startup & Enterprise Innovation:

- Startups are leveraging AI for digital twins, synthetic data generation, robotics, and automotive applications.
- Enterprise solutions include intelligent storage for AI data processing and scalable multi-agent platforms that drive asset management and ecommerce innovation.

Multimodal & Embodied AI Developments:

- End-to-end VLA models and mobile-agent technologies are advancing in-car assistants and autonomous systems.
- Innovations such as multimodal retrieval-enhanced chatbots and low-latency multimodal models (e.g., UFO-Lite) are pushing the boundaries of human-machine interaction.

Inference Optimization & Efficiency:

- Techniques like speculative sampling, communication overlap, and 2-bit post-quantization are significantly boosting LLM inference speed and accuracy.
- Breakthroughs like FlashAttention-3 and advanced RAG pipelines enhance token processing and long-context retrieval performance.

Training Acceleration & Ecosystem Expansion:

- Ecosystems built on frameworks like Megatron-Core and horizontal scaling with JAX are reducing training time for large language models.
- Pretraining domain-specific foundation models (e.g., for ecommerce) is becoming a strategic focus for enterprise applications.

Robotics & Autonomous Systems:

- Advances in VLA models and embodied intelligence are critical steps toward developing autonomous physical agents and humanoid robots.
- Emerging research in generalist robotics is paving the way for intelligent agents that integrate seamlessly into both virtual and real-world environments.

Finance & Market Applications:

- AI innovations are being applied to optimize GPU matrix operations for financial analysis, quantitative risk assessment, and market trading strategies.
- Techniques in graph networks and interpretable forecasting are enhancing fraud detection and asset management.

Enterprise AI Infrastructure:

- Scalable AI platforms from tech leaders (e.g., LinkedIn, Airbnb) are incorporating high-performance inference engines and distributed agentic deployments.
- Research insights from NVIDIA and other innovators are informing next-generation architectures and GPU optimizations.

Emerging Trends & Future Directions:

- Frontier AI strategies, open-source collaborations, and integration of quantum computing are shaping the future landscape of AI.
- Emphasis on human-AI collaboration and multidisciplinary foundation models is setting the stage for next-level intelligence and innovation.


<!-- TOC --><a name="technical-focus-areas"></a>
## Technical Focus Areas

Model Optimization & Efficiency

- Speculative decoding techniques (UFO-Lite) reducing multimodal LLM latency
- 2-bit post-training quantization strategies for memory-efficient deployment
- FlashAttention-3 advancements with asynchronous low-precision processing
- Megatron-Core ecosystem for accelerated LLM training workflows
- JAX-based horizontal scaling solutions for distributed model training

Multimodal & Embodied AI
- End-to-end VLAs (Vision-Language-Action models) for autonomous systems
- Synthetic data pipelines enhancing embodied AI training efficiency
- Mobile-Agent architectures for automotive cockpit assistants
- Humanoid robot development frameworks (Mujoco-Warp/Newton collaborations)
- Distributed agentic deployments combining vision/LLM capabilities

Industry Applications
- Automotive: Multimodal RAG systems for in-vehicle chatbots
- Finance: LLM-powered market risk analysis & fraud detection systems
- E-commerce: Domain-specific foundation model pretraining strategies
- Ads: TensorRT-LLM accelerated generative recommendation engines
- Robotics: Financial service automation through embodied intelligence

System Architecture
- CPU/GPU heterogeneous computing platforms for real-time analytics
- TensorRT-LLM implementations for video quality assessment
- Hybrid-head LLM architectures balancing performance/accuracy
- xLSTM designs enabling long-context processing efficiency
- Ray framework for scaling distributed AI applications

Emerging Frontiers
- Kolmogorov networks for interpretable financial forecasting
- Writing-in-Margins (WiM) inference patterns enhancing RAG
- Quantum computing intersections with AI infrastructure
- Multi-agent platforms for enterprise financial services
- Scientific foundation models for cross-disciplinary research

Enterprise Solutions
- NVIDIA AI Enterprise integrations (Laiye AI Foundry case study)
- LinkedIn's GPU kernel optimizations for production LLMs
- Airbnb's high-performance AI application deployment patterns
- Tencent HunYuan's inference engine optimizations
- AWS-powered fraud detection architectures


---

<!-- TOC --><a name="chinese-1-100"></a>
# Chinese: 1-100

<!-- TOC --><a name="1-ai-s73846"></a>
## 1. AIâ€¯åˆ›ä¸šä¼ä¸šåœ¨ä¸­å›½çš„å‘å±•ä¸åŠ©åŠ› [S73846]
> DataMesh 
- æ•°å­—å­ªç”Ÿï¼Œå¸®åŠ©ä¼ä¸šä½æˆæœ¬æ•´åˆæ•°æ®ã€‚åˆ¶é€ ä¸šä¸ºä¸»ã€‚
- ä¸šåŠ¡é€»è¾‘ground truth + AI Omniverseæ•°å­—å­ªç”Ÿ=é™æœ¬å¢æ•ˆ
- æ—¶ç©ºä¸€è‡´æ€§æ˜¯å…³é”®

> Limx Dynamics
- åŸºäºäººç±»æ“ä½œè§†é¢‘æ•°æ®ç”Ÿæˆå¤§æ¨¡å‹çš„æœºå™¨äººå…·èº«æ“ä½œç®—æ³•ã€‚

> é€”æ·±æ™ºåˆ
- è›‹ç™½è´¨å¤šæ¨¡æ€å¤§æ¨¡å‹è¾…åŠ©è‡ªåŠ¨åŒ–è›‹ç™½è´¨å·¥ç¨‹ã€‚
- å¤šæ¨¡æ€è‡ªç„¶è¯­è¨€å¯¹è¯+å…¨æµç¨‹ä¸šåŠ¡æ¨¡å‹

<!-- TOC --><a name="2-the-embodied-end-to-end-vla-model-driven-by-synthetic-big-data-vla-s71942"></a>
## 2. The embodied end-to-end VLA model driven by synthetic big data åˆæˆå¤§æ•°æ®é©±åŠ¨çš„å…·èº«ç«¯åˆ°ç«¯ VLA å¤§æ¨¡å‹ [S71942]

> He Wang, Galbot

- Vision-Language-Action (VLA) Model. Recent progress: $\pi_0$ model, for robot control. Bottleneck: data collection.
- Galbot's alternative data approach: Synthetic data can be scalable. NaVid (RSS 2024). Incorporate Depth: NaVid-4D (ICRA 2025).
- GraspVLA. Knowledge can be shared. 3D modality enhances spatial intelligence. Emergent abilities to learn new skills.

<!-- TOC --><a name="3-mobile-agent-s72561"></a>
## 3. Mobile-Agent: æ¢ç´¢åŸºäºå¤šæ¨¡æ€æ™ºèƒ½ä½“çš„æ±½è½¦åº§èˆ±åŠ©æ‰‹æ–°æŠ€æœ¯ [S72561]

> Ji Zhang, Alibaba Qwen

- GUI Agent: ç†è§£UIã€å®šä½æ“ä½œã€å¤šæ­¥æ“ä½œè§„åˆ’åæ€ã€æ“ä½œå»¶æ—¶
- Multi agents (planning + decision + reflection) with knowledge injection + ç«¯äº‘ååŒæ¨¡å‹æ¶æ„ã€‚
- çŸ¥è¯†å¢å¼ºï¼šæ“ä½œé€»è¾‘ tipsï¼ˆæ ¹æ®ç”¨æˆ·queryåŠ è½½çŸ¥è¯†ï¼‰ + æ“ä½œæ­¥éª¤å¢å¼º shortcutsï¼ˆç†è§£UIï¼‰ï¼Œå¯ç‚¹é¤è®¢ç«è½¦ç¥¨æœºç¥¨é…’åº—ã€‚è‡ªæˆ‘è¿›åŒ–ï¼šç”¨ä¸¤ä¸ªexperience reflectorsæ ¹æ®æ“ä½œè®°å½•å’Œé”™è¯¯æ—¥å¿—ï¼Œä¼˜åŒ–æ›´æ–°tips & shortcutsã€‚
- å¤æ‚ä»»åŠ¡æ‹†è§£ï¼šseparate high level planning & low level action, improve long-horizon planning & error recovery.

<!-- TOC --><a name="4-ai-s73910"></a>
## 4. åˆ›ä¸šä¼ä¸šåœ¨ç”Ÿæˆå¼ AI åŠæœºå™¨äººæ–¹å‘çš„å®è·µä¸åˆ†äº« [S73910]

- NVIDIA NeMo: Every enterprise needs their data flywheel (continuous learning loops for AI agents). 
- Model customization: prompt engineering/learning + fine tuning (LoRA, Adapter, IA3, SFT, RLHF)

> çˆ±åŠ¨è¶…è¶Š
- å·¥ä¸šè½¦è¾†ç«¯ä¾§å¤§æ¨¡å‹Agentå·¥ä½œæµï¼ŒADASæ™ºé©¾

> Paxini
- æœºå™¨äººITPUå¤šç»´è§¦è§‰+è§†è§‰è§¦è§‰åŒæ¨¡æ€æ„ŸçŸ¥ã€‚

<!-- TOC --><a name="5-s72500"></a>
## 5. åº”ç”¨äºæ±½è½¦è¡Œä¸šèŠå¤©æœºå™¨äººçš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ¡ˆ [S72500]

> ç‹å…‰ç”«, Great Wall Motor
- å¤šè·¯å¹¶è¡Œå¬å›ï¼Œä»ä¸åŒç»´åº¦è·å–å€™é€‰æ–‡æ¡£ï¼Œmergeå€™é€‰æ± åè¿›è¡Œç²¾æ’rankï¼Œæ ¹æ®åŸºåº§å¤§æ¨¡å‹æ€»ç»“è¾“å‡ºã€‚
- Contrastive lossï¼Œæ‹‰è¿‘æ­£ç¡®æ ·æœ¬å¯¹çš„è¯­ä¹‰è·ç¦»ï¼Œæ‰©å¤§é”™è¯¯æ ·æœ¬å¯¹çš„å·®å¼‚ã€‚
- å¯¹åŸºåº§æ¨¡å‹è¿›è¡Œè’¸é¦ï¼Œå°†DeepSeek-R1æ•™å¸ˆæ¨¡å‹è¿ç§»åˆ°å­¦ç”Ÿæ¨¡å‹ä¸Šã€‚

Advanced RAG Pipeline
- Multimodal PDF Data Extraction & Document Parsing.
- Finetune embedding model for improved retrieval. E.g., automatic training of contrastive learning loss between positive/negative chunks of user query sampling, via NeMO.
- Contextual retrieval, build context for each chunk to connect the relationship of each chunk and include global information.
- Semantic splitter, use LLM to segment documents for semantically coherent content grouping.
- Query decomposition, break down difficult query to subqueries.

<!-- TOC --><a name="6-s72716"></a>
## 6. å¤§è¯­è¨€æ¨¡å‹åœ¨æ™ºèƒ½åº§èˆ±ä¸­çš„åº”ç”¨ [S72716]

> é«˜æ°, NIO

Emotional Intelligence, E2E multi-modal diaglog

<!-- TOC --><a name="7-s72635"></a>
## 7. æ¿€å‘é€šç”¨äººå·¥æ™ºèƒ½çš„åˆ›é€ åŠ›ï¼Œå¼•é¢†æ™ºèƒ½æ±½è½¦èµ°å‘æ–°çš„æœªæ¥ [S72635]

> ç‹å°åˆšï¼ŒSenseAuto

- å›é€†è¿›åŒ–ï¼šä¸æ˜¯å¬è¯çš„å·¥å…·å¾ªè§„è¹ˆçŸ©çš„åŠ©æ‰‹ï¼Œè€Œæ˜¯æä¾›ä¸»åŠ¨æ¸©æš–å…³æ€€çš„æ–°æˆå‘˜ã€‚
- Flash Decoding. Increase concurrency and improve GPU utilization.
- Segment prefill. Change management of KV cache, reuse computation results.
- Continue batching.

<!-- TOC --><a name="8-cpugpu-s71579"></a>
## 8. åŠ é€ŸæŒ‡æ ‡è®¡ç®—ï¼šCPU/GPU å¼‚æ„å®æ—¶è®¡ç®—å¹³å° [S71579]

> å‘¨å°åï¼ŒDolphinDB

- é«˜æ€§èƒ½åˆ†å¸ƒå¼æ—¶åºæ•°æ®åº“
- Pipelineæµæ°´çº¿ä¼˜åŒ–ï¼Œå¤šçº¿ç¨‹æ‹·è´ï¼Œæå‡å†…å­˜å¸¦å®½
- é›ªçƒæœŸæƒå®šä»·ï¼Œé€‚åˆGPU

<img src="https://github.com/user-attachments/assets/6a71af90-5e76-4e1b-bc13-2896902bd045" width="50%" height="50%">

- RAG: VectorDB + TextDB

<!-- TOC --><a name="9-ufo-lite-s72498"></a>
## 9. UFO-Lite: åŸºäºè‡ªæ¨æµ‹è§£ç çš„ä½å»¶è¿Ÿå¤šæ¨¡æ€å¤§æ¨¡å‹ [S72498]

> Teng Xi, Baidu. [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1727321215230001Bhv2/FinalPresPDF/S72498_1741760325928001kU80.pdf)

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å±•ç¤ºäº†å“è¶Šçš„èƒ½åŠ›å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç›®å‰çš„ MLLMs å¾€å¾€éš¾ä»¥æ»¡è¶³å¿«é€Ÿå“åº”çš„éœ€æ±‚ï¼Œæ¨ç†å»¶è¿Ÿæˆä¸ºå…¶åœ¨ç°å®åº”ç”¨ä¸­çš„ä¸€ä¸ªé‡è¦çš„æŒ‘æˆ˜ã€‚æœ¬è®²åº§å°†ä»‹ç» UFO-Liteï¼Œè¿™æ˜¯ VIMER-UFO MLLMs ç³»åˆ—çš„æœ€æ–°å¿«é€Ÿç‰ˆæœ¬ï¼Œå…¶é’ˆå¯¹ç°å®åœºæ™¯ä¸­çš„é«˜æ•ˆåŒ–éƒ¨ç½²è¿›è¡Œäº†ä¼˜åŒ–ã€‚å…·ä½“è€Œè¨€ï¼ŒUFO-Lite å¼•å…¥äº†è‡ªæ¨æµ‹è§£ç æœºåˆ¶ï¼Œæ˜¾è‘—çš„å‡å°‘äº†ç«¯åˆ°ç«¯çš„å»¶è¿Ÿï¼Œä¸”å‡†ç¡®æ€§å‡ ä¹æ²¡æœ‰æŸå¤±ã€‚å®ƒé‡‡ç”¨äº†æ–°é¢–çš„åŒ LLM ç»“æ„ï¼Œå°†è‡ªå›å½’ç”Ÿæˆä»»åŠ¡ä¸­åºåˆ—åŒ–çš„å¤šæ¬¡å‰å‘æ¨ç†å¸è½½åˆ°äº†å¿«é€Ÿåˆ†æ”¯ï¼ˆå³è‰ç¨¿æ¨¡å‹ï¼‰ï¼Œå¹¶é€šè¿‡åŸå§‹æ¨¡å‹å¯¹è‰ç¨¿åºåˆ—è¿›è¡Œå¹¶è¡Œçš„éªŒè¯ï¼Œä»¥ä¿æŒç²¾åº¦ã€‚ä¸ºäº†å®ç°è‡ªæ¨æµ‹è§£ç ï¼ŒUFO-Lite åŸºäºé‡åŒ–æ„ŸçŸ¥çš„çŸ¥è¯†è’¸é¦ï¼Œæœ‰æ•ˆåœ°å¼€å‘äº†åŒ LLM çš„å¿«é€Ÿåˆ†æ”¯ï¼Œç¡®ä¿å…¶åˆ†å¸ƒä¸åŸå§‹æ¨¡å‹ç›¸ä¼¼ä¸”å…·å¤‡è¾ƒé«˜çš„æ¨ç†é€Ÿåº¦ã€‚æ­¤å¤–ï¼ŒUFO-Lite è¿˜æå‡ºäº†åŸºäºç½®ä¿¡åº¦çš„è‡ªé€‚åº”åˆ‡æ¢ï¼Œåˆ©ç”¨åŠ¨æ€éªŒè¯çª—å£å¤§å°ï¼Œè€Œä¸æ˜¯å›ºå®šå¤§å°ï¼Œå¹¶å…¼å®¹çŸ­åºåˆ—ç”Ÿæˆã€‚UFO-Lite åœ¨ AI2D å’Œ MathVista æ•°æ®é›†ä¸Šå±•ç¤ºäº†ä¸ Qwen2-VL-7B å’Œ InternVL2-8B ç›¸å½“çš„æ€§èƒ½ã€‚åœ¨ MMMU ä¸Šï¼Œå®ƒå®ç°äº†ä¸ InterVL2-8Bã€MiniPCPMV2-2_6 å’Œ GLM-4V-9B ç›¸å½“çš„ç»“æœã€‚ä¸ä¸Šè¿°æ¨¡å‹ç›¸æ¯”ï¼ŒUFO-lite å¯ä»¥åŠ é€Ÿ 2 å€ä»¥ä¸Šã€‚æ­¤æ¼”è®²å¯ä»¥ä¸ºè¿›ä¸€æ­¥å‘å±•é«˜æ•ˆä¸”æœ‰æ•ˆçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚


- First work on MLLMs-SPD (Speculative Decoding) and orthogonal to previous work on LLMs
- Implement based on TensorRT-LLMï¼Œæè‡´æ¨ç†é€Ÿåº¦
- Can be applied to any state-of-the-art MLLMsï¼Œä»»æ„å¤§å°tokenizerã€å¯¹ä»»ä½•å¤§æ¨¡å‹éƒ½èƒ½åŠ é€Ÿä¸”ç²¾åº¦ç›¸è¿‘(é‡åŒ–+è’¸é¦)
- Enhanced with FP8 and deployed on L20
- å¤šæ¨¡æ€ï¼šå°æ¨¡å‹è‡ªå›å½’kæ¬¡è¿­ä»£ï¼Œå¤§æ¨¡å‹ä¸€æ¬¡å¹¶è¡Œå¤„ç†
- Draft-then-Verification Paradigm of Speculative Decoding å¤§å°æ¨¡å‹ååŒï¼Œäº¤æ›¿æ¨ç†
- Confidence-based Adaptive switching strategy: å°æ¨¡å‹åšé”™/å¹»è§‰äº†è¢«å¤§æ¨¡å‹ä¿®æ­£ï¼Œå°æ¨¡å‹ä¸å¤Ÿè‡ªä¿¡è®©å¤§æ¨¡å‹éªŒè¯ã€‚
- Moreover, can Quantization-Aware Knowledge Model-Distillation
- DeepSeek-V3/R1: Multi-Token Prediction (MTP)å¯ä»¥ç›´æ¥è®©å°æ¨¡å‹é¢„æµ‹é€Ÿåº¦ç¿»å€ã€‚

<!-- TOC --><a name="10-vla-s72557"></a>
## 10. VLAï¼šè¿ˆå‘è‡ªåŠ¨é©¾é©¶ç‰©ç†æ™ºèƒ½ä½“çš„å…³é”®ä¸€æ­¥ [S72557]

> Peng Jia, Li Auto. 

- VLA based on Fast and Slow thinking systems
- Sparse attention can reduce the reasoning burden, MoE can control the growth of params and increase model capability.

<!-- TOC --><a name="11-overlap-llm-s72643"></a>
## 11. ä½¿ç”¨æŠ•æœºé‡‡æ ·å’Œè®¡ç®—é€šä¿¡ Overlap æå‡ LLM æ¨ç†æ•ˆç‡ [S72643]

> Baichuan AI, [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1727598580103001ix9j/FinalPresPDF/S72643_1741684006928001iRRh.pdf)

- æŠ•æœºé‡‡æ ·ï¼šåˆ©ç”¨decodeè¿‡ç¨‹ç®—åŠ›å†—ä½™ï¼Œç”Ÿæˆå¤šä¸ªå€™é€‰tokenè¾“å…¥ç»™LLMå¹¶è¡ŒéªŒè¯ï¼Œå……åˆ†ä½¿ç”¨ç®—åŠ›ä¸”ä¸å¢åŠ æ—¶å»¶ã€‚éš¾ç‚¹æ˜¯draft modelä¸ºå€™é€‰tokençš„äº§ç”Ÿæ–¹å¼ã€‚
  - ä½¿ç”¨æŠ•æœºé‡‡æ ·ä¼˜åŒ– decode é˜¶æ®µæ•ˆç‡é—®é¢˜ï¼Œé€šè¿‡è®¾è®¡é«˜å‘½ä¸­ç‡ä½æˆæœ¬çš„æ¨¡å‹ç»“æ„åŠåŠ¨æ€çš„å€™é€‰ token tree ç»“æ„ï¼Œæå‡æŠ•æœºé‡‡æ ·æœ‰æ•ˆæ€§
  - é‡‡ç”¨è®¡ç®—é€šä¿¡ overlap ä¼˜åŒ–é€šä¿¡å æ¯”å¤§åœºæ™¯ä¸‹ prefill æ•ˆç‡é—®é¢˜ï¼Œé€šè¿‡åˆ›æ–°çš„åºåˆ—å†… overlap æå‡è®¡ç®—åˆ©ç”¨ç‡ï¼Œä»è€Œé™ä½ prefill é˜¶æ®µè€—æ—¶ã€‚
- Clover2æ¨¡å‹ï¼Œç±»RNNæ¶æ„ï¼Œè½»é‡ã€‚èƒ½è¶…è¶ŠEagleã€‚
  - ä¼˜åŒ–loss
  - ä¸»æ¨¡å‹é¢„æµ‹tokenä¿¡æ¯å‰ç½®ï¼Œæå‰åŠ å…¥transformer blockå¸®åŠ©æ›´è¿œé¢„æµ‹
  - regressive attention block output projectorç»“æ„ç”±medusaçš„ResBlockæ”¹ä¸ºFCï¼Œæå‡åå‡ ä¸ªheadé¢„æµ‹èƒ½åŠ›
  - å¢åŠ augmenting blockå±‚æ•°ï¼Œæé«˜ä¿¡æ¯æå–èƒ½åŠ›
- å·¥ç¨‹å®ç°ï¼ˆæ¨ç†ä¸¤é˜¶æ®µï¼‰
  - Prefillé˜¶æ®µï¼ˆè®¡ç®—å¯†é›†å‹ï¼‰ï¼šè¾“å‡ºtokenç»™draft modelï¼Œdraft modelå¾ªç¯æ¯ä¸ªheadåŠ¨æ€sampleå‡ºtreeï¼Œå¯¹treeç»™decodeå¹¶è¡ŒéªŒè¯ã€‚éœ€è¦å¤šbatché«˜æ•ˆåŠ¨æ€sampleï¼Œæ„å»ºtree maskã€‚
  - Decodeé˜¶æ®µï¼ˆå†…å­˜è®¿é—®å¯†é›†å‹ï¼‰ï¼šå¹¶è¡Œæ¨ç†ç»™sampleï¼Œ ç„¶åéªŒè¯ï¼Œæœ€åä¸€ä¸ªéªŒè¯æ— æ•ˆçš„tokené‡å¤ç»™draft modelç”Ÿæˆtoken treeã€‚éœ€è¦attentionæ”¯æŒtree maskï¼ŒKV Cacheç®¡ç†ã€‚
- å·¥ç¨‹ä¼˜åŒ–
  - æ¢ç´¢ISO(åºåˆ—å†…è®¡ç®—é€šä¿¡overlap)ç­–ç•¥åœ¨LLMæ¨ç†æœŸé—´å¦‚ä½•æå‡è®¡ç®—åˆ©ç”¨ç‡ï¼Œä»è€ŒèŠ‚çœå¤§é‡é¦–tokenè€—æ—¶

<!-- TOC --><a name="12-megatron-core-s72580"></a>
## 12. æ„å»ºä»¥ Megatron-Core ä¸ºæ ¸å¿ƒçš„å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒåŠ é€Ÿç”Ÿæ€ [S72580]

> Alibaba Cloud, [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1727454091122001Qxbv/FinalPresPDF/S72580_1741685427734001itcO.pdf)

Megatron-Core æ˜¯ NVIDIA å¼€å‘çš„ç”¨äºè®­ç»ƒè¶…å¤§è§„æ¨¡ Transformer æ¨¡å‹çš„åˆ†å¸ƒå¼æ¡†æ¶ï¼Œå…·æœ‰å‡ºè‰²çš„åˆ†å¸ƒå¼æ€§èƒ½ï¼Œæ˜¯è®­ç»ƒå…·æœ‰æ•°åƒäº¿æˆ–æ›´å¤šå‚æ•°çš„å¤§è¯­è¨€æ¨¡å‹çš„å¿…å¤‡å·¥å…·ã€‚Pai-Megatron-patch æ˜¯é˜¿é‡Œäº‘æ——ä¸‹ PAI å¹³å°å¼€å‘çš„å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒå·¥å…·åŒ…ï¼ŒåŒ…å«åŸºäº Megatron-Coreæ„å»ºé«˜æ•ˆ LLM è®­ç»ƒç³»ç»Ÿçš„å…³é”®ç»„ä»¶ï¼Œå¦‚ mcore å’Œ Huggingface ä¹‹é—´çš„åŒå‘ ckpt è½¬æ¢ï¼Œå¼¥åˆ mcore å’Œ Huggingface ç”Ÿæ€ç³»ç»Ÿä¹‹é—´çš„å·®è·ï¼›å®ç°äº† Distributed Optimizer CPU å¸è½½æŠ€æœ¯ï¼Œè¿›ä¸€æ­¥é™ä½äº†å¤§æ¨¡å‹è®­ç»ƒçš„æˆæœ¬ï¼›è¿˜å¼€å‘äº†ç»™å®šç¡¬ä»¶èµ„æºæ¡ä»¶ä¸‹çš„è‡ªåŠ¨è¶…å‚æ•°ä¼˜åŒ–å·¥å…·ï¼Œæé«˜äº†æ¡†æ¶çš„å¯ç”¨æ€§ç­‰åŠŸèƒ½ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå®ƒæä¾›äº†è®­ç»ƒå„ç§å¼€æºå¤§è¯­è¨€æ¨¡å‹çš„æœ€ä½³å®è·µã€‚

- å¡«è¡¥äº† Megatron ä¼˜åŒ–å™¨åœ¨ GPU èµ„æºæœ‰é™æ—¶æ— æ³•å¸è½½çš„ç©ºç™½ã€‚ æˆ‘ä»¬å®ç°äº†å…·æœ‰é«˜æ•ˆ CPU å¸è½½çš„åˆ†å¸ƒå¼ä¼˜åŒ–å™¨ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè®­ç»ƒçš„ GPU æ˜¾å­˜ä¼˜åŒ–æŠ€æœ¯ï¼Œå¯åœ¨ GPU èµ„æºæœ‰é™çš„æƒ…å†µä¸‹è®­ç»ƒé•¿åºåˆ—
- æ”¯æŒ HuggingFace å’Œ MCore æ¨¡å‹ä¹‹é—´çš„é«˜ç²¾åº¦åŒå‘ ckpt è½¬æ¢ã€‚ æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹å¤æ‚çš„ç¨ å¯† MoE æ¨¡å‹ (ä¾‹å¦‚ Llama 3ã€DeepSeek-V2-MoE æˆ– Qwen-MoE ç­‰) è¿›è¡Œ 3D (å¼ é‡å¹¶è¡Œ/æµæ°´çº¿å¹¶è¡Œ/ä¸“å®¶å¹¶è¡Œ) è½¬æ¢
- é’ˆå¯¹çƒ­é—¨çš„ç¨ å¯† MoE æ¨¡å‹å‡çº§å’Œç®€åŒ–è®­ç»ƒæœ€ä½³å®è·µï¼Œæä¾›æ˜“äºä½¿ç”¨å¹¶ä¸”éå¸¸å¼ºå¤§çš„åŠ é€ŸæŠ€æœ¯ï¼Œä¾‹å¦‚ FlashAttention-3ã€TP-Comm-Overlapping



<!-- TOC --><a name="13-tensorrt-llm-s74181"></a>
## 13. é‡å¡‘çŸ­è§†é¢‘è§†è§‰ä½“éªŒï¼ŒåŸºäº TensorRT-LLM åŠ é€Ÿçš„æ™ºèƒ½è§†é¢‘è´¨é‡è¯„ä»·ä¸å¤„ç†å¤§æ¨¡å‹ [S74181]

> Kuaishou

å¼•å…¥Consistency Modelé€šè¿‡è’¸é¦ï¼Œæ¨ç†ä»…éœ€1æ­¥ï¼ˆdiffusion modelå¾ˆæ…¢æ¨ç†è¦25æ­¥ï¼ŒåŠ é€Ÿäº†25å€ï¼‰ï¼šOne-step inference of PTM, Compute distillation loss, EMA weight updating. 

<!-- TOC --><a name="14-laiye-ai-foundry-nvidia-ai-enterprise-s72276"></a>
## 14. Laiye AI Foundry - NVIDIA AI Enterprise åœ¨ä¸­å›½çš„æœ€ä½³å®è·µ [S72276]

> Laiye AI, [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1726190594557001j0HX/FinalPresPDF/S72276_Laiye%20AI%20Foundry_NVIDIA%20AI%20Enterprise_1741682513554001WKy5.pdf)

- Hybrid RAG = GraphRAG (Node Embedding/PageRank) + VectorRAG (semantic chunking)
- PEFT enhancements: Adaptive Rank on LoRA, Adapters Diversifying, Hybridingï¼ˆå­¦ä¹ ä¸“æœ‰çŸ¥è¯†+å…±æœ‰çŸ¥è¯†ï¼Œæ¯ä¸ªä¸“å®¶ä¸“æ³¨è‡ªå·±é¢†åŸŸçŸ¥è¯†ï¼‰

<!-- TOC --><a name="15-llm-2-bit-s72647"></a>
## 15. LLM 2-bit åé‡åŒ–çš„åŠ é€Ÿä¸éƒ¨ç½²å®è·µ [S72647]

> ByteDance, [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1727624187326001lFnY/FinalPresPDF/S72647_1741682615101001YdeF.pdf)

- LLM é«˜ç²¾åº¦ 2-bit æƒé‡å‹ç¼©ã€‚å°†æ¨¡å‹å‚æ•°è§£è€¦ä¸ºæ•´æ•°å’Œæµ®ç‚¹éƒ¨åˆ†ï¼Œå¹¶é€šè¿‡å¯¹æƒé‡å’Œ scale/zp çš„äº¤æ›¿è¿­ä»£è¿›è¡Œä¼˜åŒ–ï¼Œæ±‚è§£å±€éƒ¨æå°å€¼ï¼Œæœ€ç»ˆåœ¨ Llama-1/2 7B~70B çš„ 2-bit åé‡åŒ–ä¸Šå®ç°äº† sota ç²¾åº¦ã€‚
- 2-bit å†…å­˜è®¿é—®ä¼˜åŠ¿çš„æ–°æŠ€æœ¯ï¼Œå¹¶åŸºäº TensorRT-LLM ä¸­çš„ w4a16 Gemm è¿ç®—ç¬¦å¼€å‘ Gemm CUDA å†…æ ¸ï¼Œè¯¥å†…æ ¸å¯ä»¥é«˜æ•ˆåŠ é€Ÿ w2a16 æ¨¡å‹çš„æ¨ç†ï¼Œå¹¶åœ¨ NVIDIA GPU ä¸Šå®ç° 1.4 å€è‡³ 1.7 å€çš„åŠ é€Ÿã€‚

<!-- TOC --><a name="16-s72639"></a>
## 16. é¢å‘æµ·é‡æ¨¡å‹ä¸šåŠ¡åœºæ™¯çš„æ–‡ç”Ÿå›¾é«˜æ•ˆæ¨ç†åŠ é€Ÿè§£å†³æ–¹æ¡ˆ [S72639]

> Alibaba Cloud

Stable Diffusionå¼•å…¥latent spaceåœ¨VAEå‡å°‘è®¡ç®—ï¼Œé€Ÿåº¦æé«˜7å€ã€‚Pipelineä¸‰ä¸ªéƒ¨åˆ†ï¼šæ–‡æœ¬ç¼–ç å™¨CLIPã€VAEæ½œç©ºé—´å›¾åƒç¿»è¯‘ä¸ºè¾“å‡ºå›¾åƒã€UNetå¤šæ­¥æ¨ç†æ¨¡å‹ï¼ˆ95%è®¡ç®—é‡ï¼Œè¦ç®—å­æ€§èƒ½ä¼˜åŒ–ï¼šçŸ©é˜µä¹˜å’Œå·ç§¯ï¼‰ã€‚

<!-- TOC --><a name="17-s74073"></a>
## 17. ä¸‹ä¸€ä»£ç”Ÿæˆå¼æ¨èæ¨¡å‹è®­æ¨å¼•æ“çš„å»ºè®¾å’Œè½åœ°å®è·µ [S74073]

> Meituan, [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1734091150868001dcjp/FinalPresPDF/S74073_1741683377945001YCts.pdf)

- ä¸‹ä¸€ä»£æ¨èæ¨¡å‹ï¼ŒGennerative Recommenders (GR) Model: HSTU (Hierarchical Sequential Transduction Units)ï¼šæŠŠå¬å›å’Œæ’åºéƒ½åµŒå…¥è¿™ä¸ªåºåˆ—ç”Ÿæˆæ¨¡å‹ï¼Œå°†é•¿åºåˆ—è¾“å…¥Transformerè¿›è¡Œä¿¡æ¯äº¤äº’ã€‚
- ç¾å›¢æ¨¡å‹å·¥ç¨‹ä¼˜åŒ–ï¼š
  - æ··åˆå¹¶è¡Œç­–ç•¥ = Sparseæ¨¡å‹å¹¶è¡Œ + Denseæ¨¡å‹å¹¶è¡Œ
  - åŠŸèƒ½æ”¯æŒï¼šåŠ¨æ€hash tableæ”¯æŒåŠ¨æ€æ‰©å®¹ï¼ˆå¦‚å•†å“é¢‘ç¹ä¸Šä¸‹çº¿ï¼‰ï¼Œæ¢¯åº¦ç´¯ç§¯ï¼ˆå¤§batch sizeè®­ç»ƒï¼‰
  - æ€§èƒ½ä¼˜åŒ–ï¼šè‡ªåŠ¨åˆè¡¨ï¼Œæ··åˆç²¾åº¦è®­ç»ƒï¼Œkernelä¼˜åŒ–ï¼Œå˜é•¿åºåˆ—è´Ÿè½½å‡è¡¡ï¼ˆç”¨æˆ·åºåˆ—æ˜æ˜¾é•¿å°¾åˆ†å¸ƒï¼‰ã€‚
  - Inferenceä¼˜åŒ–ï¼šUser Cache åŒä¸€ç”¨æˆ·ä¸åŒrequestä¹‹é—´åºåˆ—ç‰¹å¾åŸºæœ¬ç›¸åŒå¯å¤ç”¨ï¼Œå…ˆè¯»ç”¨æˆ·KV Cacheå†è®¡ç®—attentionã€‚
- æœªæ¥ï¼šonline learningåœºæ™¯ã€Embeddingå‹ç¼© + ä½ç²¾åº¦è®­ç»ƒä¼˜åŒ–æ€§èƒ½ã€æ¨¡å‹å¹¶è¡Œæ”¯æŒæ›´å¤§è§„æ¨¡GRã€AOT compileæ”¯æŒPyTorch2

<!-- TOC --><a name="18-tensorrt-llm-s72995"></a>
## 18. åŸºäº TensorRT-LLM çš„å¹¿å‘Šåœºæ™¯ç”Ÿæˆå¼æ¨ç†åŠ é€Ÿæ–¹æ¡ˆ [S72995]

> JD, [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1727962469507001ov1v/FinalPresPDF/S72995_1741644906381001kPbt.pdf)

å•èŠ‚ç‚¹ï¼Œç®—åŠ›é‡Šæ”¾
- TensorRT-LLMæœ€ä½³å®è·µï¼Œè€—æ—¶çº¦æŸä¸‹ååé‡æœ€ä¼˜åŒ–
- åŸºäºå¹¿å‘ŠåœºåŸŸç‰¹æ€§ä¼˜åŒ–ï¼Œæœ‰é™è¡¨å¾ä¸‹ååä¼˜åŒ–

åˆ†å¸ƒå¼ï¼Œè½¯ç¡¬ååŒ
- å¼‚æ„ç¡¬ä»¶åˆ†å¸ƒå¼æ¨ç†ï¼ŒCPU&GPUå¼‚æ„éƒ¨ç½²
- KV Memory Toolï¼ŒKVä¸­å¿ƒï¼Œä¸‰çº§ç¼“å­˜ï¼Œå¼‚æ­¥åŠ è½½ï¼Œå¢é‡æ›´æ–°

---

<!-- TOC --><a name="english-101-200"></a>
# English: 101-200

<!-- TOC --><a name="101-enable-intelligent-storage-to-process-data-for-ai-applications-s71937"></a>
## 101. Enable Intelligent Storage to Process Data for AI Applications [S71937]

> Vincent Hsu, IBM Storage CTO. [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1726155198693001lYd6/FinalPresPDF/S71937_1741204556803001g36M.pdf)

Content-aware Storage (CAS) GenAI semantic search: Unstructured data is unsearchable, but we can integrate GenAI into our storage, so as not to repeatedly copying data from vector store all the time with lots of money.

<img src="https://github.com/user-attachments/assets/0fdbfdff-55f5-44dc-a807-87016653ce9b" width="80%" height="80%">

<img src="https://github.com/user-attachments/assets/5d41a8af-31ff-48de-b095-ff4fa955e260" width="80%" height="80%">


<!-- TOC --><a name="102-mistral-ai-placing-frontier-ai-in-your-hands-s73942"></a>
## 102. Mistral AI: Placing Frontier AI in Your Hands [S73942]

> Arthur Mensch, Mistral AI CEO

Enterprise AI strategy
- Private architecture:
  - SaaS-like simplicity on-premises and in-VPC. Everything is stateful.
  - Deep observability and robust tooling.
  - Continuous infra optimization.
<img src="https://github.com/user-attachments/assets/bad09fb3-6651-4505-b975-f4ab65958b23" width="80%" height="80%">

- E2E customizability pipeline:
  - Custom models (full/continued pretraining).
  - Rigorous post-training (SFT, DPO, alignment). 
  - Plug-n-play platform modules.
<img src="https://github.com/user-attachments/assets/d58400a0-c272-4e14-9ef0-bbcc75339227" width="80%" height="80%">

- Enterprise context: connectors
  - Integrations to enterprise knowledge.
  - Continually updating context.
  - Easy-to-build agents.
<img src="https://github.com/user-attachments/assets/3afe719a-17d6-4b28-9906-3575eb4f0116" width="80%" height="80%">

Mistral Small 3.1, OCR and SOTA embedding models 
- Clever routing between Small and Large allows to bring the best user experience
- AI app building is about stitching bricks together: bring these bricks to customers and help them stitch it.
<img src="https://github.com/user-attachments/assets/196ed814-396e-4099-8713-1c95d5651c42" width="80%" height="80%">

<!-- TOC --><a name="103-how-to-ace-a-finance-developer-interview-a-deep-dive-into-gpu-matrix-optimization-s73619"></a>
## 103. How to Ace a Finance Developer Interview: A Deep Dive into GPU Matrix Optimization [S73619]

> Joe Stam, Jump Trading, Head of Research Technology. [Slides1](https://static.rainfocus.com/nvidia/gtcs25/sess/1729878854303001ra7O/DraftPres/S73619_draft.pdf_1738621036627001aBsx.pdf), [Slides2](https://static.rainfocus.com/nvidia/gtcs25/sess/1729878854303001ra7O/supmat/MatMulSlides_old_for_v100_1729878858947001rxoU.pdf).

Key Takeaways
- Explore computation of expected speed-of-light performance of an algorithm on a particular GPU
- Optimization strategies for matrix multiplication
- Proper use of GPU memory hierarchy

Summary
- Many $O(n^3)$ problems are actually $AO(n^3) + BO(n)$, where $B \gg A$. Must fuse operations, eliminate memory trips.
- Solution: Use Shared Memory.
  - Shared memory is divided into 32 banks.
  - If multiple threads within a warp access the same bank, the warp will serialize.
  - Important when we need to handle different transpose orientations
- We need to use SMEM to batch incoming data. To saturate FMA, we need to batch multiple computations and with `.reuse`, maybe want multiple CTAs per SM. We want to use asynchronous copies.
- Performance keys: All GMEM coalesced. Use SMEM, no bank conflicts. Use Async Load to SMEM LDGSTS, 128 bit where possible. Free warp scheduler to issue FMAs - use 128 bit loads to registers. User register banking. Repeat Registers.

<!-- TOC --><a name="104-gtc-2025-keynote-s72484"></a>
## 104. GTC 2025 Keynote [S72484]

> Jensen Huang, NVIDIA CEO

- Every industry and company in the future that has factories will have two factories: factory for what they built, and factory for mathematics/AI.
- Use AI to recreate AI: Distillation process bootstraps a policy model, but complex scenarios require further tuning. Closed-loop training enables fine-tuning of policy models, digital twin and synthetic data generation enhances adaptability to diverse problems.
- Inference at scale is extreme computing, to maximize the Pareto Frontier curve of smart AI fast response + tokens per second throughput. Traditional LLMs are fast but wrong, reasoning models like DeepSeek-R1 produce more tokens thinking with good result.
- NVIDIA Dynamo, operating system of AI factory: Prefill is busy when doing deep research, and it's not generating many tokens. Decode is busy when chatting. So depending on workload, we dynamically put GPU to prefill/decode more or less, by pipeline parallel, expert parallel, tensor parallel, in-flight batching, disaggregated inferencing, and KV Cache (to route to the right GPU and manage through all the memory hierarchies).
- Pareto Frontier curve (flops * bandwidth): We need a programmable architecture that's as homogeneously fungible as possible, because the workload changes dramatically across the entire frontier.
- Storage will be reinvented from retrieval-based to semantic-based, and GPU-accelerated.
- Groot N1 for robots: Dual-system architecture for thinking fast and slow.

<!-- TOC --><a name="105-ai-for-safe-and-efficient-trading-in-electronic-markets-s72692"></a>
## 105. AI for Safe and Efficient Trading in Electronic Markets [S72692]

> Iain Dunning, Hudson River Trading, Head of AI Lab

- Provide liquidity and price discovery. Market data, beyond tick-level data order books, there's news feeds, announcements, alternative data.
- Optimize money rather than prediction results, so we have RL. RL targets real thing, but has lower signal to noise, and less compute efficiency. RL trains and tests the same thing, no need for splitting training and testing datasets.
- Build market digital twins, factory of market data, training models that learn intuitively to diverse datasets to be robust.
- LLM to predict next token (market price in next second, ~170B price tokens for 3K stocks 10 years tick data), generate multiple possible future trajectories by sample-backin sample-backin repeatedly.
- Desirable properties: probabilistic, non-degenerate, capture tails, likelihood estimation, understandable.
- Challenges: Alt data, Coherence, Tails (tail output with human-driven signals of how model behaves), Calibration, Actions, Inference.
- Massive parallel compute, on model training, model inference, simulation and generation.
- Backtest how different actions would affect market if taking them. Time horizon from intraday to single days.

<!-- TOC --><a name="106-nvidia-nventures-showcase-ai-agents-in-physical-and-virtual-worlds-dd73694"></a>
## 106. NVIDIA Nventures Showcase: AI Agents in Physical and Virtual Worlds [DD73694]

> Jim Fan, NVIDIA Principal Research Scientist. Justin Johnson, World Labs Co-founder. Misha Laskin, Reflection AI CEO. Saad Godil, Hippocratic CTO. Pete Florence, Generalist AI CEO. Eric Jang, 1x Technologies VP of AI.

- World Labs: Allow people to generate worlds by inputting single image, and generate 3D consistent world for agents to move around and interact with visually.
- Reflection AI: Today's agent is still driven by person, how to get systems that autonomously reason and backtrack on their own, and self improve to solve tasks. Coding agent is important as embodied factor for agent computer to solve general problems.
- Hippocratic: Future is vertical models, copilot is not full potential of GenAI, we need Autopilot to provide abundance in scale. Another bet is voice.
- 1x Technologies: Let AI to think harder why they fail.

<!-- TOC --><a name="107-google-ai-research-progress-and-the-future-of-ads-s73303"></a>
## 107. Google AI: Research, Progress, and the Future of Ads [S73303]

> James Johnsrud, Google Research. Tris Warkentin, Google DeepMind.

- Use RL for cold-start challenge to help advertisers better optimize bids
  - Online Bidding under RoS Constraints without Knowing the Value
- Pushing the envelope of what's possible with LLMs, modeling architectures inspired by biology (store new info as long-term memory in neuron)
  - Titans, Learning to Memorize at Test Time

<!-- TOC --><a name="108-frontiers-of-ai-and-computing-a-conversation-with-yann-lecun-and-bill-dally-s73208"></a>
## 108. Frontiers of AI and Computing: A Conversation With Yann LeCun and Bill Dally [S73208]

> Yann LeCun, Meta Chief AI Scientist, NYU. Bill Dally, NVIDIA Chief Scientist.

- 4 interesting problems: Understand physical world, have persistent memory, reason and plan.
- Token = finite set of discrete possibilities, but we don't know how to produce tokens with video (high dimensional and continuous), plus lots of things are unpredictable by pixel level (raw representation) but rather in latent space.
- Plan and Reason: We need a predictor, given state of the world, an action we imagine, can predict the next stage of the world, so we can plan a sequence of actions to arrive at outcome.
- Foundation models will be open-sourced, trained in distributed fashion around the world, having access to different subsets of data, and training a consensus model.
- Business model for AI startups: build specialized system for vertical applications, download Llama and fine-tune on proprietary data not to upload.
- Tradeoffs between training time and inference time: We expect a new model for reasoning in abstract space, because some reasoning thoughts are not related to language (token space) but rather done in abstract mental space.
  - JEPA, Joint Embedding Predictive Architecture, world models that learn abstract representations, capable of manipulating abstractions, reason and produce sequences of actions.
  - Use world model to do task, if zero-shot never encountered such task before, you don't have to be trained for this task, you can just do it without learning anything but just basic understanding of the world and planning abilities. If you do the tasks many times, it gets compiled into policy in your reactive system.
  - Two reasoning systems:
    - Automatic subconscious reactive policy (LLM can do).
    - Understand physical world. (We need a different architecture, not generative architecture, because language is discreet and simple, but the world is complicated) 
- Masked Autoencoder (MAE) reconstructs the full image from the corrupted/masked transformed version, taking both versions run through encoders and we train the representation of the full image from the representation of the corrupted one. It's joint embedding predictive architecture, with 2 latent spaces for 2 input clusters.
  - Cheaper for photos but failure for videos. VJEPA: predict on video, but at representation level, take a sliding window of 16 frames on video, can predict next few frames by minimizing prediction error (infant baby cognition).

<!-- TOC --><a name="109-building-a-scalable-enterprise-multi-agent-platform-for-financial-services-s72854"></a>
## 109. Building a Scalable Enterprise Multi-Agent Platform for Financial Services [S72854]

> Pedro Vicente, BlackRock, Aladdin Principal AI Engineer. Shawn Simpson, BlackRock, AI Labs Senior Data Science Team Director. 

- Portfolio analysis with Aladdin Copilot, Agentic platform

<img src="https://github.com/user-attachments/assets/e5eb2335-0a66-4234-8075-9cc06b591dc2" width="80%" height="80%">

- Agents Orchestration Evaluation: Configuration layer can simulate user's environment up to the application, its context, history and expected response. Solution layer can solve the task with multiple steps separate non-related threads.
- E2E Eval is a fundamental part of CI/CD pipeline, can diagnose issues in day-to-day dev.
- NVIDIA NIM - Llama 3.3 70B. NeMo Guardrails offers easy management of prompts and configs, LangGraph creates complex workflows (full control over flow and state + custom communication protocol makes interacting with multi-agents workflows in any framework).

<!-- TOC --><a name="110-distributed-agentic-multi-modal-llm-deployments-s72654"></a>
## 110. Distributed Agentic Multi-Modal LLM Deployments [S72654]

> Jeff White, Dell Product CTO.

- Combine Generative, Discriminative, Casual techniques.
- Architecture focuses on federated learning to improve the performance of distributed system.
- Continuous federated learning reduces anomalies, and take response with perfect context to RAG database, which is hard for heuristic methods without federated learning.
- Future enterprise architectures for edge AI inference will be multi-modal. With core foundation model (FM)/LLM functions centralized (attention, MLP-FFNN, layer normalization, residual connection) and some functions distributed to edge (retrieval-augmented generation, input processing-tokenization/vector embedding/position encoding [maybe], guardrails, caching, inference optimization-sampling/beam search). In addition, in multi-modal architectures, convolutional neural networks (CNNs) will be integrated with the FM/LLM.

<img src="https://github.com/user-attachments/assets/c052e6d5-b90a-43b9-82ca-b156576e4b92" width="80%" height="80%">

<!-- TOC --><a name="111-ai-agents-in-production-insights-and-future-directions-s72884"></a>
## 111. AI Agents in Production: Insights and Future Directions [S72884]

> Harrison Chase, LangChain CEO

- Key Concepts
  - Chain = predetermined control flow, sequence of steps.
  - ReAct Agent = LLM running in a loop (e.g. with for loop, tool calling, observe response)
  - Planning step: Tree of Thoughts. Reflection step: Verbal RL.
  - Flow Engineering = Pre-processing + code iterations, with reasoning models
- LangGraph: Balancing agent control with agency.
  - Controllability: to define both explicit and implicit workflows
  - Persistence: to enhance human-agent/multi-agent interactions & fault tolerance
  - Human-in-the-loop: to facilitate human guidance
  - Streaming: to expose any event or token as it occurs
- LangGraph tools
  - LangGraph platform: enterprise use for multiple teams (streamline) across the org.
  - LangGraph builder: generate scaffolding and implement business logics
  - LangGraph templates
  - LangSmith: unified testing & observability platform for improving visibility and monitoring/debugging LLM apps.
    
<img src="https://github.com/user-attachments/assets/6405ec66-46ce-4e4e-868b-f01ce47febee" width="80%" height="80%">

- Memory is different from knowledge: knowledge is fixed source of docs/info, memory is updates through user interactions.
  - 3 types
    - Semantic facts (things I learned in school)
    - Episodic experiences (thigns I did)
    - Procedural instructions (instincts or motor skills)
  - Memory is not just RAG, it uses LLM to reflect on user interactions. Agent UX is tightly coupled to memory in need of human feedback. 
  - Memory and prompt optimization are two sides of the same coin.
  - Agent UX is tightly coupled to memory in need of human feedback.
  - Long-term memory is nice-to-have and we still want low-level control over memory.
- Agent Protocol: interoperatability for LLM multi-agents, compatibility for:
  - Agents built with LangGraph or other frameworks
  - Homegrown agents
  - Vertical off the shelf vendor provided agents
- Multi Agent Prebuilt Architectures. Prebuilt packages: Supervisor and Swarm
- Blueprint: Structured report generation. Stacks to overcome challenges with agents in enterprise
  - NVIDIA NIMs: cheaply access best open source models
  - LangGraph: controllable agent framework, to perform well
  - LangGraph Platform: Agent-specific infra for deployment and management
  - LangSmith: Observability and evaluation for agents 
  
<img src="https://github.com/user-attachments/assets/ffd34136-bad1-4495-8430-16f217b89ba0" width="80%" height="80%">


<!-- TOC --><a name="112-insights-from-nvidia-research-s73202"></a>
## 112. Insights From NVIDIA Research [S73202]

> Bill Dally, NVIDIA Chief Scientist.

<!-- TOC --><a name="gpu-performance-bottlenecks-for-llm-workloads-with-optimizations-for-memory"></a>
### GPU performance bottlenecks for LLM workloads with Optimizations for Memory
- LLM training and inference prefill phase (parallel processing on input prompt tokens) are bound by computing large GEMM kernels in FFN layers
  - Solution: Cash-Aware GEMM kernels (CAGE), reordering dataflows in GEMM kernels to maximize data locality in L2 cache, to make better use of L2 cache leads to better power efficiency & performance for large GEMM kernels
- LLM inference decode phase (sequentially generating output tokens) is bound by Key-Value (KV) cache loarding in attention layers
  - Solution: RocketKV for training-free KV cache sparsity, for accurate estimate on critical KV cache tokens via multi-stage (prefill phase to coarse-grain permanent KV cache eviction, decode phase to fine-grain dynamic KV token selection), multi-dimensional approximation. 

<!-- TOC --><a name="hybrid-head-llms-and-hymba-architecture"></a>
### Hybrid-head LLMs and Hymba Architecture
<img src="https://github.com/user-attachments/assets/b8be9674-375e-4732-963c-6ef3b747fb2b" width="80%" height="80%">

<img src="https://github.com/user-attachments/assets/1749172b-3ffa-4d3d-9151-af2d1b3f9c04" width="80%" height="80%">

- Hybrid LLM: Hymba Architecture
  - Combined attention and state-space heads. Better results by combining layers in parallel, take state space model in pink, and regular transformer, we have sliding window layer (less expensive transformer), sharing the KV-Cache.

<img src="https://github.com/user-attachments/assets/237f6afb-08ac-41c2-bb83-db69682cde85" width="80%" height="80%">

- Attention much more efficient. Sliding window attention with state space head, we have much more full attention.

<img src="https://github.com/user-attachments/assets/b548836b-d7c5-4c64-b407-8f8c10a61c1d" width="80%" height="80%">

<img src="https://github.com/user-attachments/assets/eec6f61f-da52-473b-b252-bb96a93f0271" width="80%" height="80%">

<img src="https://github.com/user-attachments/assets/d7d84588-ae74-423f-a76b-0f05bbcb064c" width="80%" height="80%">


<!-- TOC --><a name="113-scaling-vision-llms-to-extract-and-deploy-targeted-metadata-for-catalog-management-s73701"></a>
## 113. Scaling Vision LLMs to extract and deploy targeted metadata for Catalog Management [S73701]

> Shopify

- Two stages of serving in production (classification then extract): low latency real time prediction, high throughput/volume near real time prediction (less strict SLA with ability for in-flight batching, and LLM as classifier).

<img src="https://github.com/user-attachments/assets/78f75cd9-34b1-4c42-a107-790dd0bf13a4" width="80%" height="80%">

<!-- TOC --><a name="114-productionize-llms-for-quantitative-analysis-of-market-risk-an-exploratory-attempt-s73818"></a>
## 114. Productionize LLMs for Quantitative Analysis of Market Risk: An Exploratory Attempt [S73818]

> Dimitris Emmanoulopoulos, Barclays, Head of Machine Learning Technologies

- Experiment: Meta-Llama-3-70B, input tokens: 2048, output tokens: 1024, FP16, NVIDIA NIM: TensorRT-LLM
- Risk Workflow: Can use any LLM, OCR, Retriever, because NIMs containerize them.
<img src="https://github.com/user-attachments/assets/98865447-d920-49a2-80f2-761eb636a7d6" width="50%" height="50%">
<img src="https://github.com/user-attachments/assets/def1aa26-c761-4876-baff-5638743d0c2c" width="50%" height="50%">

<!-- TOC --><a name="115-an-introduction-to-building-humanoid-robots-s72590"></a>
## 115. An Introduction to Building Humanoid Robots [S72590]

> Jim Fan, Yuke Zhu. NVIDIA

- Groot N1: combination of continuous diffusion and discrete LLM, and combination of Dual Systems: System 1 (low-level closed-loop sensorimotor control), System 2 (high-level cognition, reason and plan).

<img src="https://github.com/user-attachments/assets/ea3b26f4-734a-4b72-8b7c-e6044ce94df3" width="80%" height="80%">

<img src="https://github.com/user-attachments/assets/7f39497c-c872-4f43-96cf-205da7f94248" width="80%" height="80%">


<!-- TOC --><a name="116-leveraging-large-model-based-embodied-intelligence-to-enhance-financial-service-robots-s71267"></a>
## 116. Leveraging Large Model-Based Embodied Intelligence to Enhance Financial Service Robots [S71267]

> Jianzong Wang, Ping An Technology Deputy Chief Engineer. [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1725419593222001E00c/FinalPresPDF/S_71267_1741673499511001z8i1.pdf)

<img src="https://github.com/user-attachments/assets/e2c40869-f53e-404b-98f0-28adf2fd63d9" width="80%" height="80%">


<!-- TOC --><a name="117-ai-for-humanoid-robots-s73182"></a>
## 117. AI for Humanoid Robots [S73182]

> Pieter Abbeel, UC Berkeley

Body Transformer (BoT), Embodiment-aware architecture for policy learning
- A spatially modular architecture in place of a monolithic NN, can capture spatial dependencies in high-dimensional settings
  - inductive bias
  - computationally efficient
  - enable multi-frequency reasoning
  - localized "credit assignment"
- By exploiting the sparse nature of architecture, BoT achieves 2-3x reduction in computation cost. Improves imitation learning performance and scalability, reinforcement learning and transfers to real-world. 
<img src="https://github.com/user-attachments/assets/3ff646fc-c224-4d2a-a380-d00f541a4740" width="80%" height="80%">

- HumanoidBench
- Addressing high-dimensional action space: Hierarchical learning for long-horizon reasoning can solve complex tasks.

MuJoCo
- Can we train RL policies in simulation for safe and cheap data? MuJoCo playground, full-stack robotics suite, open-source library for GPU-accelerated robot learning and sim-to-real transfer, can even train on Colab notebook.
- Training process: Define task and (many) rewards. Starting training in simulation (PPO) > tune the rewards. Define learning curriculum. Add domain randomization. Transfer to real world.
- Limiations: JAX computation time scales with # possible contacts. JIT compilation slow. Reward shaping is still important.

<!-- TOC --><a name="118-jane-street-how-an-early-ai-adopter-thinks-about-infrastructure-presented-by-coreweave-s74219"></a>
## 118. Jane Street: How an Early AI Adopter Thinks About Infrastructure (Presented by CoreWeave) [S74219]

> Adam Canady, Jane Street. Peter Salanki, CoreWeave CTO. [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1734382333688001wR6T/FinalPresPDF/S74219_1741645197269001Gqhs.pdf)

- Harnessing Cutting-Edge GPUs for Financial Innovation: CoreWeaveâ€™s infrastructure delivers Jane Street reliable access to the latest NVIDIA GPUs, enabling rapid training and fine-tuning of AI models to drive innovation in financial markets
- Scaling Infrastructure for Demanding AI Workloads: CoreWeave proactively builds out GPU capacity and optimizes data center designs, including liquid cooling and RDMA networking, to meet the unique challenges of large-scale workloads
- Ensuring Reliability in Mission-Critical Applications: CoreWeaveâ€™s automated node health checks, robust architecture, and continuous monitoring provide Jane Street with the stability needed to maintain uptime in the high-stakes world of quantitative trading

## The Next Era of Payments: How Generative AI is Shaping the Future [S72846]

<!-- TOC --><a name="119-the-promise-of-humanoid-robots-research-vs-the-real-world-s72592"></a>
## 119. The Promise of Humanoid Robots: Research vs. the Real World [S72592]

> Aaron Saunders, Boston Dynamics CTO. [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1727465141102001g41h/DraftPres/NVIDA_GTC_2025_Saunders_1741990990349001f3fa.pdf)

<img src="https://github.com/user-attachments/assets/fc9b6a73-994e-4207-9701-6e5396d5fe17" width="80%" height="80%">

<img src="https://github.com/user-attachments/assets/ce42b2b0-455c-4316-885f-a501d9c44f57" width="80%" height="80%">


<!-- TOC --><a name="120-a-new-era-of-generalist-robotics-the-rise-of-humanoids-s72543"></a>
## 120. A New Era of Generalist Robotics: The Rise of Humanoids [S72543]

> Deepak Pathak, Skild AI CEO. Tiffany Janzen, Tiffintech Founder. Jim Fan, NVIDIA Principal Research Scientist. Aaron Saunders, Boston Dynamics CTO. Pras Velagapudi, Agility Robotics CTO. Bernt BÃ¸rnich, 1X CEO.

- Jim Fan on Groot's strategy: Model should be simple, while data pipeline (all that surrounded by model) should be complicated. All complicated data strategies, we compress them into one clean artifact suffices for a wide range of tasks.
- Bernt BÃ¸rnich: Intelligence comes from diversity, which is important for robots to understand tasks. Robots can learn its own dynamics history.
- Deepak Pathak: It's not that language leads to intelligence, but it's infrastructure exists in our brain came to physical reasoning. Let robots learn by experience, rather than control robots. Robot is learning agent, learning on the fly, which is different from train-deploy schemes in other AI/LLMs. 
- Jim Fan on future 5-20 years: Embodied scaling law for robotics. Robotics-accelerated and -automated science. Robotics automating robotics itself and self-improve recursively (like LLM's AutoML, prompt LLMs to deeper research finding next Transformer).

<!-- TOC --><a name="121-tencent-hunyuan-building-a-high-performance-inference-engine-for-large-models-based-on-nvidia-tensorrt-llm-s71563"></a>
## 121. Tencent HunYuan: Building a High-Performance Inference Engine for Large Models Based on NVIDIA TensorRT-LLM [S71563]

> Yifu Sun, Tencent Applied Researcher. [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1726046708673001W9i7/FinalPresPDF/GTC-yifu-Tencent%20HunYuan%20Building%20a%20High-Performance%20Inference%20Engine%20for%20Large%20Models%20Leveraging%20NVIDIA%20TensorRT-LLM_1741670208137001HVLE.pdf)

- Performance Optimization: Strategies for effectively tuning TensorRT-LLMâ€™s out-of-the-box performance to the maximum, along with customized optimization approaches
- Model Compression Techniques: An exploration of various methods for model compression, and the tangible benefits derived from their implementation
- Parallel Inference: Strategies for achieving high-performance inference of ultra-large mixture-of-experts models across multiple GPUs and nodes
- KV-cache Optimization: Efficient generation and loading of KV-cache, in conjunction with algorithmic customizations
- Service Scheduling Strategies: Implementation of flexible service scheduling strategies based on NVIDIA Triton, enabling both continuous batching and prefill/decode separation

<img src="https://github.com/user-attachments/assets/5f256ab0-d284-4e13-bf66-c8eb72877eee" width="80%" height="80%">

<img src="https://github.com/user-attachments/assets/87347234-3d7f-49d0-8771-9171e95e9a7a" width="80%" height="80%">

<img src="https://github.com/user-attachments/assets/b12ef26d-8c3c-4504-bcee-4bc2fe1ac961" width="80%" height="80%">

<!-- TOC --><a name="122-accelerate-super-long-context-llm-inference-s72568"></a>
## 122. Accelerate Super Long-Context LLM Inference [S72568]

> Boyuan Huang, Alibaba Cloud Product Director. [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1727451588919001blpQ/FinalPresPDF/GTC2025_v1.1_1741657177860001TsJU.pdf)

BladeLLM for long-context and MoE models
- Optimized Sparse Attention kernel, intensively optimized loading of sparse FP4 KV pairs from global memory.
- Accelerate FFN with memory-bounded MoE decoding.
- Dynamic Chunked Pipeline Parallelism
- Distributed Inference: Multi-node Prefill-Decode DisAggregation

<!-- TOC --><a name="123-scaling-ai-platform-at-linkedin-llms-agents-gpus-kernels-and-more-s72963"></a>
## 123. Scaling AI Platform at LinkedIn: LLMs, Agents, GPUs, Kernels, and More [S72963]

> Animesh Singh, LinkedIn AI Executive Director. [Informative Video](https://register.nvidia.com/flow/nvidia/gtcs25/vap/page/vsessioncatalog/session/1727943593183001B2E4)

- Scaling AI Infra
<img src="https://github.com/user-attachments/assets/5f144c0f-0c75-47c5-a14c-cce7275a6453" width="80%" height="80%">

- LinkedIn GPU Efficient Runtime (Liger) kernel: efficient triton kernels for LLM training
<img src="https://github.com/user-attachments/assets/c203bad7-2805-46be-bd02-6dc904115b4c" width="80%" height="80%">

- Distributed training with DeepSpeed ZeRO++

<img src="https://github.com/user-attachments/assets/17e5c3f0-01c7-405d-9e84-476664f5760f" width="80%" height="80%">

<img src="https://github.com/user-attachments/assets/983f49a1-ed43-47b7-8ad5-ad9d4b17e4b2" width="80%" height="80%">

- Data loading with Torch Avro
- Use Case: 360 Brew, distilled from foundation model.

<img src="https://github.com/user-attachments/assets/96d1dda6-0c29-446d-940e-0a0a00083837" width="80%" height="80%">

- Multi-Agent Orchestration

<img src="https://github.com/user-attachments/assets/92ebc66f-6037-4490-b164-b04b6b580a73" width="80%" height="80%">

<img src="https://github.com/user-attachments/assets/0573ffdf-6fbc-4071-912f-d14ac3e17a69" width="80%" height="80%">

- Incremental Training

<img src="https://github.com/user-attachments/assets/4ce9c459-7739-4e96-a1d7-56aabbca9c14" width="80%" height="80%">

- GNN Training

<img src="https://github.com/user-attachments/assets/d7f85ae3-ecca-4a83-9fbf-9b9906f0d0d1" width="80%" height="80%">

- Workflow orchestration: Flyte

<img src="https://github.com/user-attachments/assets/0a633370-d8b3-49ef-ab73-da9953ad87bb" width="80%" height="80%">

<img src="https://github.com/user-attachments/assets/9b3c48c7-c16e-4f94-ab8c-6cdca2b82fe0" width="80%" height="80%">

- AI Pipelines

<img src="https://github.com/user-attachments/assets/37a920f1-fd18-45a3-afa8-8cb4a7e2a96f" width="80%" height="80%">

<!-- TOC --><a name="124-streamlining-investment-insights-for-wealth-management-with-generative-ai-s71653"></a>
## 124. Streamlining Investment Insights for Wealth Management with Generative AI [S71653]

> Orest Xherija, UBS Director

Evaluation and Guardrail models for RAG
- LLM-as-a-Judge critic ensembling for robust evaluation of RAG with NeMo Evaluator
- Inference-time control by fine-tuning small models with online feedback monitoring and real-time blocking of hallucination output

<img src="https://github.com/user-attachments/assets/11acae22-2667-4972-918d-7ac5f0f34113" width="80%" height="80%">

<!-- TOC --><a name="125-unlocking-the-future-of-llms-high-efficiency-xlstm-architectures-for-scalable-performance-s71812"></a>
## 125. Unlocking the Future of LLMs: High-Efficiency xLSTM Architectures for Scalable Performance [S71812]

> Sepp Hochreiter, NXAI Chief Scientist. [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1726132188712001KFs0/FinalPresPDF/S71812_1741601965571001LoNm.pdf)

- LSTM limitations
  - Can't revise storage decisions, can only do nearest neighbor search. Solution: exponential gating
  - Limited storage capacities, rare token predictions. Solution: Matrix memory & covariance update
  - No parallelization for training. Solution: update is memory independent.
- xLSTM (7B model) can beat Transformer, and is the fastest model, can deal with long context processing.
- Can be used for predictive analysis, robotics and real-time simulations.
- Chunk-wise kernel is faster than flash-attention.

<!-- TOC --><a name="126-the-future-of-ai-scaling-intelligence-open-source-innovation-and-human-ai-collaboration-s73863"></a>
## 126. The Future of AI: Scaling Intelligence, Open-Source Innovation, and Human-AI Collaboration [S73863]

> Ali Farhadi, Allen Institute for AI CEO

- How to evaluate/score the exploring space of trajectories and exhaustive CoT of AI reasoning? Simply generating more/repeated tokens is not effective reasoning, we should incorporate structured way as part of training.
- Hard-core reasoning should have applications in science, e.g. discovery, solving contradictions, use literature to generate new hypothesis, but these abilities don't show up in our benchmark test as they're hard to evaluate.
- We're in evaluation crisis, overfitting in benchmarks and improvements in models don't mean improvements in real world. Any evaluation put out there can be saturated and fulfilled quickly if focused on overfitting it, let alone covering a wide spectrum is hard and expensive.
- We see drifts of fine-tuning models, especially if we don't know the distribution of pretrained data, then how to collaborate with others effectively? From checkpoint to checkpoint, what happened to the model?

## How Intercontinental Exchange is Taking AI from Production to Scale to Improve Efficiency [S72463]



<!-- TOC --><a name="127-announcing-mujoco-warp-and-newton-how-google-deepmind-and-nvidia-are-supercharging-robotics-development-s72709"></a>
## 127. Announcing Mujoco-Warp and Newton: How Google DeepMind and NVIDIA are Supercharging Robotics Development [S72709]

> Erik Frey, Google Lead Researcher. [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1727738299460001toka/FinalPresPDF/erikfrey%20GTC%20Presentation_1742536120007001LdsB.pdf)

MuJoCo algorithms require branching, but dynamic (fine-grained) branching in JAX is tricky. So we introduce Mujoco-Warp.

<!-- TOC --><a name="128-unlocking-high-performance-ai-applications-at-airbnb-s73265"></a>
## 128. Unlocking High-Performance AI Applications at Airbnb [S73265]

> Airbnb

- Kubernetes cluster: Triton model (ensemble, pre/post processors, in-flight batching, multi-gpus, numerous backends)
<img src="https://github.com/user-attachments/assets/fb0b3771-a6c6-4d71-a1f0-d0d130746d40" width="80%" height="80%">


<!-- TOC --><a name="129-flashattention-3-fast-and-accurate-attention-with-asynchrony-and-low-precision-s71368"></a>
## 129. FlashAttention-3: Fast and Accurate Attention With Asynchrony and Low Precision [S71368]

> Tri Dao, Together AI Chief Scientist. [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1725825619590001romQ/FinalPresPDF/S71368_FlashAttention-3-%20Fast%20and%20Accurate%20Attention%20With%20Asynchrony%20and%20Low%20Precision_1742587734302001Zsa8.pdf)

Attention, as a core layer of the ubiquitous Transformer architecture, is the bottleneck for large language models and long-context applications. FlashAttention pioneered an approach to speed up attention on GPUs by minimizing memory reads/writes. However, it has yet to take advantage of recent hardware. We develop three main techniques to speed up attention on NVIDIA Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and interleave block-wise matmul and softmax operations, and (2) block quantization and incoherent processing that leverages hardware support for FP8 low-precision. Our method, FlashAttention-3, achieves speedup on H100 GPUs by 1.5-2.0X, with BF16 reaching up to 800 TFLOPs/s, and with FP8 reaching1.3 PFLOPs/s. We validate that FP8 FlashAttention-3 achieves 2.6X lower numerical error than a baseline FP8 attention.

- Existing attention algorithms such as FlashAttention(-2) have yet to take advantage of new hardware features such as asynchrony and low precision in Hopper
- Asynchrony allows us to overlap computation and data movement, speeding up attention computation
- Low precision (FP8) speeds up attention even further, and with careful quantization techniques one can reduce numerical error
- Consumer-Producer pipelines with GEMM-Softmax Overlapping, combining both pingpong scheduling and intra-warpgroup overlapping to define FA-3 computation path.
- Load balancing for causal attention. Flash Decoding: split along the KV sequence length to occupy the GPU with enough work.
- Multi-head Latent Attention (MLA) in DeepSeek has large head dim, standard splitting doesn't have enough registers: Warp specialization for large head dim.

FlashAttention-3 Summary
- Fast, accurate attention
- Asyncrhony, low-precision
  - Persistent kernels with LPT scheduling for causal attention
  - For inference: Split KV (Flash Decoding), GQA packing

<!-- TOC --><a name="130-scaling-and-leveraging-multi-modal-large-models-for-robloxs-metaverse-s72911"></a>
## 130. Scaling and Leveraging Multi-Modal Large Models for Roblox's Metaverse [S72911]

> Denis Goupil, Roblox Principal ML Engineer

<img src="https://github.com/user-attachments/assets/fea5e041-651f-47b2-ac21-70bee6f385b5" width="80%" height="80%">

- GPUs as pets (from sheep to cattle), and use Yunikorn to increase GPU efficiency.

<img src="https://github.com/user-attachments/assets/1377b94e-d93d-4a51-9652-d0c802c8cb50" width="80%" height="80%">

<!-- TOC --><a name="131-quantum-computing-where-we-are-and-where-were-headed-s74495"></a>
## 131. Quantum Computing: Where We Are and Where Weâ€™re Headed [S74495]

> Jensen Huang, NVIDIA CEO. Alan Baratz, D-Wave CEO. Ben Bloom, Atom Computing CEO. John Levy, SEEQC CEO. Krysta Svore, Microsoft Technical Fellow. LoÃ¯c Henriet, Pasqal CEO. Matthew Kinsella, Infleqtion CEO. Mikhail Lukin, QuEra Computing. Pete Shadbolt, PsiQuantum CEO. Peter Chapman, IonQ Executive Chair. Rajeeb Hazra, Quantinuum CEO. Rob Schoelkopf, Quantum Circuits Chief Scientist. Simone Severini, AWS. Subodh Kulkarni, Rigetti CEO. ThÃ©au Peronnin, Alice & Bob CEO.

- GenQAI: Quantum as an additional tool of instrument into maturing , e.g. Quantum computing as an accelerator/augmentation for Agentic AI computaion. Classic and Quantum computers compute in 2 different ways, with input and output for each other, where output of quantum computer is input for LLM. So that LLM understand the ground state energy, ground state configuration of molecules, then we can use quantum to produce highly accurate/predictive ground-truth similated/training data (e.g. for protonation, drug paradigm analysis, etc) for classical model.
- Multi-scale challenge: Rate of scaling is not limited by Moore's Law in Quantum. Latency and Throughput requirements together is super challenging.
- Catching the flywheel (solving problem better than others, high volume, debudget, and get higher volume and so on) to get there, is not easy in a high computation requirement as it may be impossible to scale for classical, the whole point is to invent new algorithms.
- Topological Quantum Computing, Microsoft. Quantum AI factories in Physical AI as catalyst to compute nature.
- Jensen's closing remark: Quantum computer is not to replace classical computer, but it's QPU that's added to CPU/GPU to extend classical computing to do things otherwise cannot. Use Quantum computer to make classical computer way better to create ground truth in physics, biology, chemistry, and use AI to solve drug discovery.

<!-- TOC --><a name="132-peering-into-the-future-what-ai-and-graph-networks-can-mean-for-the-future-of-financial-analysis-s74726"></a>
## 132. Peering Into the Future: What AI and Graph Networks Can Mean for the Future of Financial Analysis [S74726]

> Sudeep Kesh, S&P Global CIO. [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1739906058885001OxEF/FinalPresPDF/S74726_1742490911406001cFdM.pdf)

- E2E data pipeline to build bi-partite graph representation for discovering relationships in financial market data.
<img src="https://github.com/user-attachments/assets/b1beaf78-bede-4797-9bc2-9225b36afb54" width="80%" height="80%">

- Capture macroeconomic conditions to predict bond issuance.
<img src="https://github.com/user-attachments/assets/ab2a4148-a3e4-46c1-87a3-b66290eb90e2" width="80%" height="80%">

- GNN Graph as novel forecasting method. Model training on quarterly data with walk forward validation. Can predict politics?

<img src="https://github.com/user-attachments/assets/be049fab-0832-49ed-8ffe-c7efe8b78cb2" width="80%" height="80%">



<!-- TOC --><a name="133-horizontal-scaling-of-llm-training-with-jax-s73266"></a>
## 133. Horizontal Scaling of LLM Training with JAX [S73266]

> Google Cloud. [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1728073363553001mzFs/FinalPresPDF/S73266%20_%20Horizontal%20Scaling%20of%20LLM%20Training%20with%20JAX_1742516812327001pRhl.pdf)

- Circular pipeline parallelism to reduce bubble
<img src="https://github.com/user-attachments/assets/2bdef2c0-76d0-4178-82d3-1c4f861b4186" width="80%" height="80%">


<!-- TOC --><a name="134-toward-multidisciplinary-scientific-foundation-models-s72256"></a>
## 134. Toward Multidisciplinary Scientific Foundation Models [S72256]

> French National Center for Scientific Research (CNRS). [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1726185731094001emx3/FinalPresPDF/S72256_Toward%20Multidisciplinary%20Scientific%20Foundation%20Models_1742514322588001DpXi.pdf)

<img src="https://github.com/user-attachments/assets/2c359da7-13e9-45f9-9a3f-508e1ca69ebd" width="80%" height="80%">

- AION-1: AstronomIcal Omnimodal Network, The First Scientic Large Multimodal Data Model
<img src="https://github.com/user-attachments/assets/51f59823-b648-4532-8b8a-7c918586193c" width="80%" height="80%">

<img src="https://github.com/user-attachments/assets/e41cdfe3-9bfe-4613-8df2-46635bf0f6b3" width="80%" height="80%">

<img src="https://github.com/user-attachments/assets/d40fd255-999b-4b7e-a68f-0367907e1155" width="80%" height="80%">


<!-- TOC --><a name="135-kolmogorov-ai-networks-for-interpretable-financial-forecasting-s72542"></a>
## 135. Kolmogorov AI Networks for Interpretable Financial Forecasting [S72542]

> Yigal Jhirad, Cohen & Steers Head of Quantitative and Derivatives Strategies. [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1726089157740001BBQM/supmat/cufolio-DLI_1726089161827001Boes.pdf)

- Forecasting volatility is the key for option pricing, portfolio construction and optimization (covariance matrices), and effective strategies like risk parity and volatility targeting for variance swaps and VIX futures.
- S&P 500 autocorrelation of monthly volatility is positive and persists overtime. Vol is mean-reverting and clustering (monthly vol transition matrix). Returns and vol are negatively correlated.
- KAN is interpretable and parameter-efficient, as any multivariate function can be decomposed to a sum of univariate (spline) functions. It has great transparency to facilitate informed investment decisions.
  - smooth returns while preversing trends and capturing asymmetry
  - log-like compression to control for extremity/overreaction
  - amplify small credit shifts where spread widening is amplified vs. tightening.
  - These are summed up to forecast vol and capture nonlinear dependencies.
- For feature engineering not to be highly correlated, we use Shapley regression and decision trees to identify data inputs (term structure, credit spreads, sector indexes), training data 2005-2021, testing data 2022-2025, to forecast next 10 days vol.
- KAN training pipeline: dynamic online learning approach using roll windows to adapt to varying market conditions, pipeline and feature transformers compatible with RAPIDS cuML and PyTorch.
- Reframing problems as 1D curves for inputs and outputs can simplify modeling process. Symbolic representation KAN can analyze high dimensional scenario generation, with high accuracy and shallow architectures.
- Ray tune for distributed hyperparameter optimization (HPO), 1000 experiments to tune KAN architecture and learning params on training and validation. Challenges with pykan during HPO: NaNs with LBFGS and Adam optimizers when employing large values for grid refinement of splines.
- Ensemble KAN using 100 random seeds to forecast from 2022 onwards. Further work on regime-based models and integration of KAN as part of a broader ensemble of models.

![image](https://github.com/user-attachments/assets/b923d901-e532-4706-9b84-cd15bc3b875d)


<!-- TOC --><a name="136-scaling-distributed-ai-applications-with-ray-s71766"></a>
## 136. Scaling Distributed AI Applications with Ray [S71766]

> Stephanie Wang, Anyscale Founding Engineer. [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1726120281556001UcZH/FinalPresPDF/S71766%20Scaling%20Distributed%20AI%20Applications%20with%20Ray_1742582757547001g3ge.pdf)

- Why Compiled Graphs (static task graphs that can allocate resource once and reuse for multiple executions across multiple GPUs)?
  - Ray Core's classic API encourages dynamic task creation, there's cost of RPC and dynamic memory allocation.
  - Developing Multi-GPU Applications: Ray Compiled Graphs gives you the flexibility of the Ray API when programming multi-GPU applications. Express arbitrary execution graphs across different devices and execute with 10s of us of task overhead
- Why GPU objects (optimized for distributed RLHF)?
  - Dynamic single-controller orchestration of GPU-GPU communication reduce complexity w.r.t. compiled graphs API.
  - Native GPU-GPU Communication: Ray Compiled Graphs uses NVIDIA NCCL for GPU-GPU communication for fast Tensor transfer. The system guarantees no deadlock and implements optimizations such as overlapping communication with computation
  - LLM Inference: Ray Compiled Graphs will be the default distributed execution backend for vLLM v1, supporting Tensor-parallel and pipeline-parallel LLM inference
  - Distributed Training Flexibility: Ray Compiled Graphs make it simple to build out complex distributed training tasks such as heterogeneous multimodal training and RLHF.
<img src="https://github.com/user-attachments/assets/a85f460c-7c9d-44e8-a6e6-1e5916ceb9c3" width="80%" height="80%">

<!-- TOC --><a name="137-startup-pitches-generative-ai-llms-and-data-s74529"></a>
## 137. Startup Pitches: Generative AI, LLMs, and Data [S74529]

- LinqAlpha CEO: AI compound system equipped with finance-trained AI agents, to synthesize insights from fragmented information and unstructured alternative proprietary data, and to generate alpha with one-stop AI platform for hedge funds.

<img src="https://github.com/user-attachments/assets/6a7f09d7-40b7-4fea-a61d-1276de3a1419" width="80%" height="80%">

<img src="https://github.com/user-attachments/assets/c5bb98ca-afba-44e5-bb89-6b315b8fa350" width="80%" height="80%">

[Poster: Linq-Embed-Finance](https://static.rainfocus.com/nvidia/gtcs25/sess/1729702534460001tf4h/poster/P73504-Finance-Specific%20Embeddings%3A%20Achieving%20Precision%20in%20Financial%20Data%20Search_1742353691013001RbJT.PDF)

<img src="https://github.com/user-attachments/assets/20f28e72-9ef0-4ea1-ab49-3492ca2fd7a4" width="80%" height="80%">

- FalkorDB CEO: GraphRAG builds a real-time knowledge graphs for GenAI apps, with real-time contextual reasoning.
  - Sparse matrices to represent graph topology & linear algebra for graph traversal, with memory efficient optimized algorithms and GPU parallelized and vectorized.
  - Use cases: LLM-enhanced reasoning, fraud detection, recommendation engines, visualize codebases as knowledge graph to analyze dependencies, bottlenecks, and optimize projects.


<!-- TOC --><a name="139-from-rag-to-agents-building-enterprise-products-with-generative-ai-s71685"></a>
## 139. From RAG to Agents: Building Enterprise Products with Generative AI [S71685]

> Dropbox.

<!-- TOC --><a name="140-advanced-rag-pipelines-engineer-scalable-retrieval-systems-for-enterprise-ai-s73015"></a>
## 140. Advanced RAG Pipelines: Engineer Scalable Retrieval Systems for Enterprise AI [S73015]

> Deepset. [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1727971295926001T5M8/FinalPresPDF/S73015_Advanced%20RAG%20Pipelines-%20Engineer%20Scalable%20Retrieval%20Systems%20for%20Enterprise%20AI%20_1742604996740001ZZQw.pdf)

- Haystack: Open-source LLM orchestration framework. Building blocks = custom components & custom RAG pipelines (directed graphs).
- Hayhooks: tool to deploy and serve Haystack pipelines as REST APIs. Pipeline -> Endpoint.

- Advanced RAG: multiple retrieval methods combined
<img src="https://github.com/user-attachments/assets/b48436c0-5edd-4b55-a96a-91da1774f0fc" width="80%" height="80%">

<img src="https://github.com/user-attachments/assets/75e8dd8d-16c7-460e-8144-6b19e84e90dc" width="80%" height="80%">


<!-- TOC --><a name="141-enhance-llms-with-writing-in-the-margins-wim-a-better-inference-pattern-for-long-context-retrieval-s72323"></a>
## 141. Enhance LLMs With Writing in the Margins (WiM): A Better Inference Pattern for Long-Context Retrieval [S72323]

> Waseem Alshikh, Writer CTO. [Slides](https://static.rainfocus.com/nvidia/gtcs25/sess/1726246419320001YBXO/FinalPresPDF/S72323_1741999284107001GsWo.pdf), [Demo](https://www.youtube.com/watch?v=84YUIaZTRas)

- Writing in the margin as chunk-wise processing with intermediate margins streaming, and better inference pattern for long contextual retrieval, just like ã€Šçº¢æ¥¼æ¢¦ã€‹è„‚ç šæ–‹æ‰¹ã€‚

<img src="https://github.com/user-attachments/assets/1c96ec0a-d532-45e1-a944-b6eb42e5495a" width="80%" height="80%">

- Line 6 & 9: If you don't know the answer, if text is not related, if text doesn't mention the person by name: answer No. If you can extract relevant info, answer Yes. 
<img src="https://github.com/user-attachments/assets/00e0609a-e156-4a7e-b40f-00d33c9c72af" width="60%" height="60%">


---

<div align="left">
  <marquee behavior="alternate" scrollamount="3">
    <strong>Connect with me:</strong>
    &nbsp;
    <a href="https://github.com/junfanz1">GitHub</a> â€¢
    <a href="https://www.overleaf.com/read/jcgfkzhyfvdv#57139d">Resume</a> â€¢
    <a href="https://www.linkedin.com/in/junfan-zhu/">LinkedIn</a> â€¢
    <a href="https://x.com/junfanzhu98">X</a> â€¢
    <a href="mailto:junfanzhu98@gmail.com">Email</a> â€¢
    <a href="junfanzhu98">WeChat</a>
  </marquee>
</div>
