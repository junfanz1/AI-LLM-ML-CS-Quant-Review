# 30 Takeaways from Shunyu Yao's Talk on Agentic AI

Shunyu Yao has been one of the most influential researchers shaping the modern agentic AI movement. From ReAct to multi-agent collaboration, his work at OpenAI has deeply impacted how we think about reasoning, planning, and human-AI interaction. 

This post summarizes key takeaways from his latest in-depth 3-hour interview [https://www.youtube.com/watch?v=gQgKkUsx5q0&t=3245s], where he reflects on 6 years of agent research, the boundaries between humans and systems, and the future of a world that is simultaneously unified and pluralistic. 

A few thoughts before reading:

ğŸ’¬ How far can reasoning take agents beyond todayâ€™s chatbots?

ğŸ”‘ Can we design rewards that are clear & hack-proof â€” or is reward hacking inevitable?

ğŸ”„ Will startups win by inventing new interaction modes or by building better data flywheels â€” or both?

ğŸ‘‘ Will the future of AI be dominated by super-apps or decentralized multi-agent systems?

ğŸ” What â€œcontrarian betsâ€ could define the next scaling dimension of AI?

# English

## Definition of Agent
- **Agent**: interacts with the external world, makes decisions autonomously, and optimizes for reward.
- Early symbolic agents were rule-based, but rules can never capture every detail and edge case in the real world.
- **Deep Reinforcement Learning (RL)**, such as AlphaGo, leveraged infinite playthroughs in virtual environments, rewards, and general network architectures to learn like a black box.  
  - Limitation: environment-specific engineering (e.g., the game itself), poor generalization to new environments/games, and limited applicability in the real world.
- **Large model reasoning** allows agents to operate in new environments â€” coding, the internet, and the real world.
- **Language agents** are more fundamental than other agents:  
  - Language models provide powerful *priors*, enabling reasoning.  
  - Reasoning enables generalization â€” a requirement for thinking and problem-solving.

## Two Main Directions of Agents
- **Agents with their own rewards and exploration.**
- **Multi-agent systems** that form organizations.

### Improving Agent Capabilities
- Better context handling.
- Stronger memory.
- Lifelong online learning.

## Coding as an Affordance
- Coding is like the **hands** of a human â€” the most important affordance for AI.
- APIs are part of code. Future AGI may be API/code-based, human-defined, or a mixture of both.
- Many things donâ€™t have APIs â€” itâ€™s like asking whether we should make cars drive on all roads (agent adapts) or redesign all roads to fit cars (environment adapts).
- Ultimately, agents may be capable of doing almost everything.

## Generalization
- If pretraining includes all possible knowledge, RL is just activating skills.
- The optimal generalization may look like "overfitting reality" â€” if an agent is truly general, overfitting vs. generalizing stops mattering.
- Generalization comes from **reasoning**: transferable thinking skills across environments.
- Often, AI does not lack skills but lacks the **full context** to apply them.

## Second Half: Current State of Language Agents
- We now have general methods. The bottleneck is no longer training models but defining **good tasks** and **good environments**.
- **Key to RL success**: a well-defined task.
  - Rewards should be **outcome-based**, **computable**, and **not noisy**.
  - Process-based rewards or preference-based rewards risk **hacking** and do not solve the real problem.

### Types of Tasks
- **Reliability-oriented tasks** (customer support).
- **Creativity-oriented tasks** (writers, mathematicians).
- **Multi-agent tasks** emphasize breadth and long-term memory (like a company vs. a single person).
- **Design considerations**:
  - For code: measure *pass@1*, *pass@100* (at least one success).
  - For reliability tasks: measure probability of *never failing* or *failing at most once*.
- Current systems do not pay enough attention to **robustness** in simple tasks.

## Startups & Interfaces
- Startups must design **new interaction interfaces** *and* ensure the model is capable enough to power them. Both are necessary.
- If you only use old interfaces, ChatGPT will replace you.
- If you build new interfaces without better models, you will struggle.
- **Super Apps** are a double-edged sword: once you choose one interface, most of your resources will follow that path (e.g., Googleâ€™s search engine path dependency).

## Data Flywheel
- Most companies **do not have a data flywheel** and rely on better models.
- A data flywheel requires:
  - Training your own models.
  - Interaction-driven rewards to separate good data from bad data.
- **Good example: Midjourney**
  - Clear rewards (user likes/dislikes).
  - Aligned with the companyâ€™s business.
  - Creates a data flywheel â€” but note, this is not the main business for most companies.

## Pretraining & RL
- Pretraining may or may not be necessary â€” depends on the gap between open-source and closed-source models.
- Pretraining is a **non-trivial foundation for RL**:
  - Allows RL to work in language/prior-knowledge-based environments without "10^30 years" of learning.
- **Cost-value tradeoff**:
  - Current pretraining is expensive with limited value.
  - Justifiable when the environment is closed, high-value, and forms a full feedback loop (e.g., Google Ads).
  - But many real-world tasks are long-tail and require generalization, making pretraining more important.

## Research Directions for Agents
- **Long-term memory/context**: some contexts are distributed in the brain and cannot be written down.
- **Intrinsic rewards**: innovation requires intrinsic motivation, not just external rewards.
  - Babies explore through curiosity and internal drives.
  - Human games are closer to "word games," while baby games are physical exploration.
- **Scalable multi-agent systems**: creating new, great organizations.

## Human vs. Agent
- Humans can reason, machines are still lacking.
- Start from **first principles** and **utility** when designing systems.

## Crypto + Agents & Value-Based Business Models
- Human networks are becoming more centralized.
- Pareto principle (80/20) and Matthew effect (rich-get-richer) lead to stronger dominance of big companies.
- Centralization and diversity are not contradictory â€” both are accelerating.
- **Context limitation** is the ultimate barrier to centralization.
- Many peopleâ€™s value lies in **information asymmetry** â€” maintaining privilege requires inventing distributed networks (e.g., traders sharing information via multi-agent exchange).

## Future Outlook
- Look for **contrarian opportunities** (different bets, unique interaction paradigms).
- You donâ€™t need everyoneâ€™s consensus â€” just enough consensus.
- New **scaling dimensions** will emerge, but choosing the right scale for each application will be key.

## Language as the Core Tool
- Language is the most fundamental tool for generalization â€” like **fire** for humans.
- Specific domains (e.g., Go) may have better specialized reasoning systems, but language remains the **universal tool**.

## Scaling Up Agents
- The most important thing is to **find highly valuable applications**.
- Costs will naturally go down over time.

## Strong Agents & Interaction Modes
- There is no single definition of a "strong agent."
- Intelligence boundaries are interface-dependent, not model-dependent.
- Many interaction modes for assistants have not yet been invented.

## Manus & Killer Apps
- Manus represents a **general first-principle sense/interaction** with huge imaginative potential.
- Coexists with **killer apps** â€” doing one thing very well.

## DeepSeek & ChatGPT
- **DeepSeek**: long chains of thought are crucial â€” they give users a new experience.
- **ChatGPT**: not just technology, but memory forms a moat.  
  - More context â†’ more stickiness â†’ stronger moat.

## MCP & Context
- MCP is also memory.
- The world has a memory hierarchy â€” external environment included.
- Long context windows are a way to achieve long-term memory.

## Evaluation
- Necessary for measuring performance.
- Must be based on **real-world value**, not leaderboard scores.
- Ideally, evaluate across hundreds of tasks and aggregate rewards.
- The goal is to measure **how much better someone becomes after a year**, not just snapshot benchmarks.

## Future of Agents
- Expect **agentic interaction modes** and **new products like Cursor** built on larger, more general environments.
- Two directions:
  - **Model-based agents** (e.g., remote VMs).
  - Agents embedded into existing environments.

## Agentic Capabilities
- Need:
  - Rich interaction with digital environments (MCP, API).
  - Careful design, infrastructure, engineering.
  - An **ecosystem** for user intentions.

## Distributed vs. Centralized Agents
- In the next two years, agents may still be more centralized (super apps).
- Two directions:
  - Local user-environment optimization.
  - Creating brand-new environments (DeepResearch).

## High-Value Startup Opportunities
- Accumulating **user context** or building **specialized environments**.
- User context is like **oil before the invention of cars** â€” massive future value.
- If intelligence becomes ubiquitous, platforms like WeChat (environment + context) become powerful moats.

## Organizations: CEO vs. Scientist
- Organizational architecture can create many innovations.
- The path of organizational creation is different from that of scientific invention.

## Universal Mindset
- Focus on **high-ceiling projects**.
- Stay curious and imaginative.
- Creating something **more general than humans** is one of the most exciting goals.
- Suggested reading: *A Brief History of Intelligence*, biographies.

# ä¸­æ–‡

Agentå®šä¹‰ï¼šèƒ½å’Œå¤–ç•Œäº¤äº’ã€è‡ªæˆ‘å†³ç­–ã€optimize rewardã€‚

- æ—©æœŸç¬¦å·ä¸»ä¹‰çš„åŸºäºç¡¬è§„åˆ™çš„agentï¼Œä½†è§„åˆ™æ°¸è¿œæ— æ³•æ¶µç›–ä¸–ç•Œä¸Šæ‰€æœ‰ç»†èŠ‚å’Œç‰¹æ®Šæƒ…å†µã€‚
- æ·±åº¦å¼ºåŒ–å­¦ä¹ å¦‚AlphaGoï¼Œæœ‰æ— ç©·æ¬¡ç©çš„è™šæ‹Ÿç¯å¢ƒã€å¥–åŠ±ã€é€šç”¨ç½‘ç»œæ¶æ„ï¼Œåƒé»‘ç›’ä¸€æ ·å­¦ä¹ è¿›æ­¥ï¼Œä½†é—®é¢˜æ˜¯ç¯å¢ƒ-specificçš„å·¥ç¨‹ï¼ˆå¦‚æ¸¸æˆæœ¬èº«ï¼‰ï¼Œæ— æ³•æ³›åŒ–åˆ°å…¶ä»–ç¯å¢ƒã€æ¸¸æˆï¼Œä¸”å¾ˆéš¾çœŸå®ä¸–ç•Œåº”ç”¨ã€‚
- å¤§æ¨¡å‹æ¨ç†ï¼Œå¯ä»¥æœ‰æ–°ç¯å¢ƒå¦‚codingã€äº’è”ç½‘ã€çœŸå®ä¸–ç•Œã€‚

è¯­è¨€agentæ¯”å…¶ä»–agentæ›´æœ¬è´¨ï¼šå› ä¸ºè¯­è¨€æ¨¡å‹æä¾›äº†å¼ºå¤§çš„å…ˆéªŒçŸ¥è¯†ï¼Œå…ˆéªŒçŸ¥è¯†å¯ä»¥æ¨ç†ï¼Œæ¨ç†æ‰èƒ½æ³›åŒ–ã€‚å› ä¸ºäººå¯ä»¥æ€è€ƒï¼Œè€Œæ²¡æœ‰æ¨ç†åšä¸åˆ°æ€è€ƒèƒ½åŠ›ã€‚

ä»Šå¤©agentçš„ä¸¤ä¸ªæ–¹å‘ï¼šagentæœ‰è‡ªå·±çš„rewardå’Œæ¢ç´¢ã€multi-agentå½¢æˆç»„ç»‡ã€‚

æå‡agentèƒ½åŠ›ï¼šå¤„ç†context, memoryèƒ½åŠ›ï¼Œå¹¶åšlifelong online learningã€‚

ç¼–ç¨‹ç¯å¢ƒï¼šcodingå°±åƒäººçš„æ‰‹ï¼Œæ˜¯AIæœ€é‡è¦çš„affordanceã€‚APIä¹Ÿæ˜¯codeä¸€éƒ¨åˆ†ï¼Œæœ€ç»ˆAGIæ˜¯åŸºäºAPIæˆ–codeï¼Œè¿˜æ˜¯åŸºäºäººçš„å®šä¹‰ï¼Œå¯èƒ½æ˜¯ä¸¤è€…mixedçš„ã€‚å¾ˆå¤šäº‹æƒ…å¹¶æ²¡æœ‰APIï¼Œå°±åƒè®©è½¦å¼€åœ¨æ‰€æœ‰çš„è·¯ä¸Šï¼Œè¿˜æ˜¯ç”¨äººåŠ›æ”¹é€ æ‰€æœ‰çš„è·¯å»é€‚åº”è½¦ï¼ˆAPIï¼‰ï¼Œæœ€ç»ˆagentå¯èƒ½ä»€ä¹ˆéƒ½åšã€‚

æ³›åŒ–ï¼šè‹¥pretrainingåŒ…å«æ‰€æœ‰ä¸–ç•Œä¸Šäº‹æƒ…ï¼ŒRLåªæ˜¯æ¿€å‘å‡ºæ‰€æœ‰skillï¼Œmaybe the optimal generalization is to overfit realityï¼Œå¦‚æœæ˜¯å…¨èƒ½çš„é‚£ä¹ˆè®¨è®ºæ˜¯overfitè¿˜æ˜¯æ³›åŒ–å°±ä¸é‡è¦äº†ã€‚ä½†æ³›åŒ–çš„åŸå› æ˜¯èƒ½æ¨ç†ï¼Œå³æ€è€ƒçš„æŠ€èƒ½å¯ä»¥è¿ç§»åˆ°ä¸åŒç¯å¢ƒï¼Œè€Œä¸ä»…æ˜¯æŠ€èƒ½å¤šå°‘ã€‚å¾ˆå¤šæ—¶å€™AIæ¨¡å‹çš„æŠ€èƒ½ä¸å·®ï¼Œä½†ç¼ºä¹çš„æ˜¯å®Œæ•´çš„contextã€‚

Second Halfï¼šåŸºäºè¯­è¨€çš„agentæ­£åœ¨è½¬ç§»ï¼Œç°åœ¨å·²æœ‰é€šç”¨æ–¹æ³•ï¼Œbottleneckä¸æ˜¯è®­æ¨¡å‹å’Œæ–¹æ³•ï¼Œè€Œæ˜¯æ€ä¹ˆå®šä¹‰å¥½çš„ä»»åŠ¡ã€å¥½çš„ç¯å¢ƒï¼Œç”¨é€šç”¨æ–¹æ³•è§£å†³ä»€ä¹ˆé—®é¢˜ã€‚

RLæˆåŠŸå…³é”®æ˜¯ä¸€ä¸ªå¥½ä»»åŠ¡ï¼šæƒ³å®šä¹‰ä¸€ä¸ªrewardæ˜¯åŸºäºç»“æœè€Œéè¿‡ç¨‹ï¼Œæ˜¯åŸºäºå¯è®¡ç®—çš„è§„åˆ™è€ŒéåŸºäºäººæˆ–æ¨¡å‹çš„é»‘ç›’åå¥½ã€‚RLä»»åŠ¡æœ€éš¾çš„æ˜¯å®šä¹‰ç™½ç›’è€Œéé»‘ç›’ä¸”ä¸noisyçš„rewardï¼Œä»»åŠ¡æœ‰éš¾åº¦æœ‰ä»·å€¼ã€‚å¦‚æœåŸºäºè¿‡ç¨‹å»å®šä¹‰rewardå°±ä¼šå‡ºç°hackingï¼Œå¦‚æœä¼˜åŒ–äººæˆ–æœºå™¨çš„åå¥½ä¹Ÿä¼šå‡ºç°hackingï¼Œä¸èƒ½è§£å†³é—®é¢˜ã€‚

å®šä¹‰ä¸åŒä»»åŠ¡ï¼šæœ‰çš„ç®€å•ä½†æ³¨é‡reliabilityï¼ˆå®¢æœï¼‰ï¼Œæœ‰çš„æ³¨é‡creativityï¼ˆä½œå®¶ã€æ•°å­¦å®¶ï¼‰ï¼Œmulti-agentæ³¨é‡ä»»åŠ¡å¹¿åº¦å’Œé•¿æœŸè®°å¿†ï¼ˆä¸€ä¸ªäººå’Œä¸€ä¸ªå…¬å¸çš„åŒºåˆ«ï¼‰ã€‚è®¾è®¡æ–¹æ³•æ˜¯pass@1, pass@100ï¼Œèµ·ç æˆåŠŸä¸€æ¬¡çš„æ¦‚ç‡ï¼ˆä»£ç ï¼‰ï¼Œæ°¸è¿œæˆåŠŸæˆ–æœ€å¤šå¤±è´¥ä¸€æ¬¡çš„æ¦‚ç‡ï¼ˆå®¢æœï¼‰ï¼Œæˆ‘ä»¬ç›®å‰å¯¹ç®€å•ä»»åŠ¡çš„robustnessè¿˜ä¸å¤Ÿé‡è§†ã€‚

åˆ›ä¸šå…¬å¸ï¼šåˆ›ä¸šå…¬å¸è¦èƒ½è®¾è®¡ä¸åŒinterfaceå’Œäººæœºäº¤äº’æ–¹å¼ï¼Œå³åˆ›é€ æ–°çš„äº¤äº’æ–¹å¼ï¼ˆå¦‚Cursorå¯ä»¥æç¤ºä½ å†™ä»£ç ï¼‰&æ¨¡å‹æœ‰æº¢å‡ºèƒ½åŠ›æ¥èµ‹èƒ½è¿™ä¸ªäº¤äº’æ–¹å¼ï¼ŒäºŒè€…ç¼ºä¸€ä¸å¯ã€‚å¦‚æœåªåšæ—§çš„interfaceï¼Œé‚£å®¹æ˜“è¢«ChatGPTå–ä»£ï¼›å¦‚æœåšæ–°çš„interfaceä½†æ¨¡å‹èƒ½åŠ›æ²¡æœ‰å˜å¥½ï¼Œé‚£ä¹Ÿéš¾åšã€‚æœ‰super APPå¯¹å…¬å¸æ¥è¯´æ˜¯åŒåˆƒå‰‘ï¼Œå› ä¸ºå½“ä½ æœ‰ä¸€ä¸ªäº¤äº’æ–¹å¼çš„æ—¶å€™ï¼Œå…¬å¸çš„å¤§éƒ¨åˆ†èµ„æºå¿…ç„¶å½¢æˆè·¯å¾„ä¾èµ–ï¼ˆä¾‹å¦‚è°·æ­Œä¾èµ–æœç´¢å¼•æ“ï¼‰ã€‚

æ•°æ®é£è½®ï¼šå¤§éƒ¨åˆ†å…¬å¸æ²¡æœ‰æ•°æ®é£è½®ï¼Œä¾èµ–äºæ¨¡å‹å˜å¥½ã€‚æ•°æ®é£è½®éœ€è¦è‡ªå·±è®­æ¨¡å‹ã€é€šè¿‡äº¤äº’æœ‰å¥½rewardæ¥åŒºåˆ†å¥½æ•°æ®åæ•°æ®ã€‚å¥½æ¡ˆä¾‹Midjourneyï¼šæœ‰æ¸…æ™°rewardï¼ˆäººå–œæ¬¢å“ªå¼ å›¾ï¼‰ã€åº”ç”¨ä¸æˆ‘å…¬å¸ä¸šåŠ¡alignï¼Œè¿™æ ·å¯ä»¥æ•°æ®é£è½®ï¼Œä½†æ•°æ®é£è½®ä¹Ÿä¸ä¸»çº¿ã€‚

Pretrainingï¼šä¼šå°è¯•è®­ç»ƒæ¨¡å‹ï¼Œä½†æœªå¿…pretrainingï¼Œçœ‹æƒ…å†µï¼Œå–å†³äºå¼€æºä¸é—­æºæ¨¡å‹çš„gapã€‚Pretrainingç»™RLåšé“ºå«æ˜¯non-trivialçš„ï¼Œèƒ½åœ¨åŸºäºè¯­è¨€å’Œå…ˆéªŒçŸ¥è¯†çš„ç¯å¢ƒé‡ŒåšRLï¼ˆè€Œä¸éœ€è¦å­¦10^30å¹´ï¼‰ã€‚cost-valueï¼šå½“å‰costå¾ˆå¤§è€Œvalueä¸å¤§ï¼Œä½†ä¹Ÿè®¸ä¸åŒåº”ç”¨æœ‰ä¸åŒå½¢æ€agentï¼Œå¾ˆå¤šäº¤äº’æ–¹å¼éœ€è¦ä¸åŒæ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœèƒ½è¶³å¤Ÿjustify pretraining costï¼Œå°±ä¼šåˆç†ã€‚å¦‚æœæœ‰å°é—­ç¯å¢ƒå‚ç±»ä»·å€¼è¶³å¤Ÿå¤§ï¼Œæ•°æ®èƒ½å½¢æˆé—­ç¯ï¼Œä»…RLå°±å¯ä»¥ï¼ˆå¦‚è°·æ­Œadsï¼‰ã€‚ä½†ä¸–ç•Œæœ‰å¾ˆå¤šé•¿å°¾äº‹æƒ…ï¼Œéœ€è¦generalizationï¼Œéœ€è¦åƒäººç±»ä¸€æ ·åœ¨çº¿å­¦ä¹ å¹¶é€‚åº”ç¯å¢ƒï¼Œè¿™æ—¶pretrainingæ›´é‡è¦å› ä¸ºéœ€è¦æ³›åŒ–æ€§ã€‚

Agentç ”ç©¶æ–¹å‘ï¼šlong term memory/contextï¼ˆæœ‰äº›contextåªåœ¨å¤§è„‘ä¸­åŸºäºåˆ†å¸ƒå¼å­˜åœ¨è€Œæ— æ³•å†™ä¸‹ï¼Œå› æ­¤äººæ— å¯æˆ–ç¼ºï¼‰, intrinsic rewardï¼ˆå› ä¸ºç›´åˆ°æˆåŠŸä¹‹å‰éƒ½æ²¡æœ‰reward/feedbackï¼Œè€Œæ˜¯å†…åœ¨ä»·å€¼è§‚å’Œæ¿€åŠ±ï¼Œè¿™æ˜¯åˆ›æ–°è€…å¿…è¦çš„ã€‚å©´å„¿å¯ä»¥é€šè¿‡å¥½å¥‡æˆ–å†…åœ¨motivationåšå°è¯•ï¼Œè€Œäººç±»ç¤¾ä¼šçš„æ¸¸æˆæ›´åƒæ–‡å­—æ¸¸æˆï¼Œä¸å©´å„¿çš„ç‰©ç†æ¸¸æˆä¸åŒï¼‰, scalable multi-agentï¼ˆåˆ›é€ æ–°çš„ä¼Ÿå¤§ç»„ç»‡çš„äººï¼‰

äººä¸agentï¼šäººèƒ½æ¨ç†è€Œæœºå™¨ä¸å¤Ÿï¼ŒåŸºäºç¬¬ä¸€æ€§åŸç†è®¾è®¡ï¼Œä»utilityå‡ºå‘ã€‚

Crypto+agentï¼Œvalue-basedå•†ä¸šæ¨¡å¼ï¼šäººç±»æ˜¯ç½‘ç»œï¼Œä¸­å¿ƒåŒ–ç¨‹åº¦ä¼šå¢åŠ ï¼Œè´«å¯Œå·®è·åŠ å¤§å…·æœ‰28å®šå¾‹å’Œé©¬å¤ªæ•ˆåº”ï¼Œå¤§å…¬å¸å¯¹ä¸–ç•ŒæŒæ§å¢åŠ ã€‚ä¸è¿‡å¦ä¸€ä¸ªç»´åº¦æ˜¯ä¸­å¿ƒåŒ–ä¸diverseä¸çŸ›ç›¾ï¼Œå¦‚ä»Šäººä»¬ä»ç½‘ç»œè¾¹ç¼˜åˆ°ä¸­å¿ƒçš„è·ƒè¿é€Ÿåº¦åŠ å¿«ï¼Œæ™®é€šäººçš„æœºä¼šä¹Ÿæ›´å¤šï¼Œäº§ä¸šç»„ç»‡å½¢æ€ä¹Ÿåœ¨å¢åŠ ã€‚æŠ€æœ¯å‘å±•çš„è¶‹åŠ¿æ˜¯ä¸¤è€…éƒ½åŠ å‰§ï¼Œå› ä¸ºæ•ˆç‡æ˜¯æ ¹æœ¬åŸå› ã€‚ä¸­å¿ƒåŒ–çš„æé™æ˜¯context limitationï¼Œå¾ˆå¤šäººçš„ä»·å€¼å¹¶éæ˜¯æŸä¸ªå•ä¸€æŠ€èƒ½çš„é«˜è¶…ï¼Œè€Œæ˜¯æŒæ¡äº†åˆ«äººæ²¡æœ‰çš„ä¿¡æ¯å·®ï¼Œä¸ºäº†ç»´æŒè‡ªå·±çš„previledgeä¼šå‘æ˜å‡ºdistributed networkï¼Œä¾‹å¦‚æ¯ä¸ªtraderéƒ½æœ‰è‡ªå·±çš„ä¿¡æ¯ï¼Œç”¨multi-agentäº¤æ¢ä¿¡æ¯æ¥åšäº¤æ˜“ã€‚ç°åœ¨å¼ºå·¨å¤´æœ‰motivationåšä¸­å¿ƒåŒ–ï¼Œè€Œä¸­å¿ƒåŒ–ä»¥å¤–çš„åŠ›é‡ä¹Ÿä¼šæœ‰motivationå»åšä¸­å¿ƒåŒ–ï¼Œä¸¤ç§åŠ›é‡éƒ½ä¼šå­˜åœ¨ã€‚

æœªæ¥ï¼šè¦å¯»æ‰¾åå…±è¯†çš„äº‹æƒ…ï¼Œæœ‰different betï¼ˆå¤šç§super appçš„ä¸åŒäº¤äº’æ–¹å¼ï¼‰ï¼Œä¸éœ€è¦æ‰€æœ‰äººçš„å…±è¯†è€Œåªéœ€è¦è¶³å¤Ÿå¤šäººçš„å…±è¯†å°±èƒ½åšäº‹ã€‚æœªæ¥ä¼šæœ‰æ–°scaling dimensionå‡ºç°ï¼Œä½†åŸºäºä¸åŒåº”ç”¨å¦‚ä½•é€‰æ‹©scaleæ¯”é‡æ˜¯è¯¾é¢˜ã€‚

è¯­è¨€ï¼šæ˜¯äººä¸ºäº†å®ç°æ³›åŒ–å¹¶å®Œæˆäº‹æƒ…çš„æœ€æœ¬è´¨çš„å·¥å…·ï¼Œå°±åƒç«ï¼Œè€Œè¯­è¨€å¯ä»¥è§£å†³ä»»ä½•æ–°ä»»åŠ¡ã€‚ç‰¹å®šé¢†åŸŸï¼ˆå¦‚å›´æ£‹ï¼‰æœ‰æ¯”è¯­è¨€æ›´å¥½çš„æ€ç»´æ–¹å¼ï¼Œä½†è¯­è¨€ä¸æ˜¯ä¸ºäº†å¤„ç†ç‰¹å®šä»»åŠ¡ï¼ˆå¯èƒ½ç‰¹å®šä»»åŠ¡å­˜åœ¨å†—ä½™æ€§ï¼Œä½†æ•´ä½“é€šç”¨ï¼‰ï¼Œè€Œè¯­è¨€æ˜¯é€šè¿‡å¼ºå¤§å…ˆéªŒçŸ¥è¯†å¯ä»¥æ‰“é€šå„é¢†åŸŸçš„é€šç”¨å·¥å…·ã€‚

Agentæ€ä¹ˆscale upï¼šæ‰¾åˆ°å¾ˆæœ‰ä»·å€¼çš„åº”ç”¨æ˜¯æœ€é‡è¦çš„ï¼Œcostæ€»ä¼šé™ä½ã€‚

å¼ºagentï¼šä¸åŒäº¤äº’æ–¹å¼ä¸‹æœ‰ä¸åŒå®šä¹‰ï¼Œä¸æ˜¯å•æçš„ï¼Œéœ€è¦ä¸åŒç³»ç»Ÿå»åˆ¤æ–­ä»·å€¼ã€‚æ™ºèƒ½è¾¹ç•Œå–å†³äºäº¤äº’æ–¹å¼è€Œésingle modelï¼Œæ¯”å¦‚åšåŠ©æ‰‹è¿˜æœ‰å¾ˆå¤šæ–°çš„äº¤äº’æ–¹å¼æ²¡è¯ç”Ÿã€‚

Manusï¼šå¾ˆé€šç”¨çš„ç¬¬ä¸€æ€§æ„Ÿè§‰/äº¤äº’æ–¹å¼/å¾ˆå¤šæƒ³è±¡ç©ºé—´ï¼Œä¸æœ‰killer appï¼ˆåšä¸€ä»¶äº‹å¥½çš„åº”ç”¨ï¼‰ï¼Œä¸¤è€…ä¸çŸ›ç›¾ã€‚

DeepSeekï¼šé•¿æ€ç»´é“¾å¾ˆé‡è¦ï¼Œç»™äººæ–°ä½“éªŒã€‚

ChatGPTï¼šä¸ä»…æ˜¯æŠ€æœ¯ï¼Œè€Œä¸”memoryå¯ä»¥å½¢æˆå£å’ï¼Œç”¨æˆ·ç”¨çš„é»æ€§å¼ºï¼Œæœ‰æ›´å¤šcontextå°±æœ‰æ›´å¤šé»æ€§å’Œå£å’ï¼ˆæ¯”å¦‚æœ‰æ•ˆä»å¾ˆå¤šç”¨æˆ·å¯¹è¯ä¸­æç‚¼å‡ºç›¸å…³ä¸œè¥¿ï¼‰ã€‚

MCPä¹Ÿæ˜¯memoryï¼Œcontextåœ¨è½¯ä»¶é‡Œï¼šä¸–ç•Œæœ‰memory hierarchyï¼Œå¤–éƒ¨æ˜¯ä¸–ç•Œç¯å¢ƒã€‚

Long contextæ˜¯å®ç°long term memoryçš„æ–¹å¼ã€‚

è¯„ä¼°ï¼šæ˜¯è¡¡é‡å¥½åçš„å¿…è¦æ¡ä»¶ï¼Œè¯„ä¼°å–å†³äºç°å®ä¸–ç•Œçš„å®é™…ä»·å€¼ï¼Œè€Œéåˆ·æ¦œä¸Benchmarkã€‚å®šä¹‰è€ƒè¯•å’Œæ¸¸æˆæ˜¯ç®€å•well-defined rewardï¼Œè€Œä¸–ç•Œå¾ˆéš¾æ˜¯å› ä¸ºæ²¡æœ‰æ ‡å‡†ç­”æ¡ˆã€‚è¯„ä¼°å¯èƒ½è¦å–å†³äºå‡ ç™¾ä¸ªä»»åŠ¡ï¼ŒæŠŠå¹³è¡Œçš„æ•°æ®åŠ åœ¨ä¸€èµ·è¿›è¡Œrewardï¼Œä¾‹å¦‚ï¼Œè¯„ä¼°äººä¸€å¹´åèƒ½å˜å¾—å¤šå¥½ï¼Œè€Œä¸æ˜¯åœ¨100ä¸ªå¹³è¡Œå®‡å®™ä¸­èƒ½å˜å¾—å¤šå¥½ã€‚

Agentçš„æœªæ¥ï¼šä¼šæœ‰agenticäº¤äº’æ–¹å¼ï¼Œæœ‰æ–°çš„cursoräº§å“å‡ºç°ï¼Œä½†åŸºäºæ–°çš„æ›´å¤§çš„ç¯å¢ƒcopilotï¼Œä¸¤æ–¹é¢ï¼šåŸºäºæ¨¡å‹çš„ï¼ˆå¦‚remote virtual machineï¼‰ï¼Œæˆ–è€…æ—¢æœ‰çš„ç¯å¢ƒåœºæ™¯ä¸­æŠŠagentå¼•å…¥ã€‚

Agenticå¢å¼ºèƒ½åŠ›ï¼šagentå’Œæ•°å­—ä¸–ç•Œçš„äº¤äº’ç¯å¢ƒï¼ˆMCPï¼ŒAPIï¼‰ï¼Œäººå’Œagentäº¤äº’æ˜¯ä»€ä¹ˆã€‚éœ€è¦å¾ˆå¤šè®¾è®¡ã€infraã€å·¥ç¨‹ã€‚è¿˜æœ‰ï¼Œå¦‚ä½•æ„å»ºuser-intentionç”Ÿæ€ç³»ç»Ÿã€‚

Agentéœ€è¦è™šæ‹Ÿæœºå—ï¼šä¸¤å¹´å†…å¯èƒ½è¿˜ä¸ä¼šé‚£ä¹ˆåˆ†å¸ƒå¼ï¼Œè€Œæ˜¯æ›´ä¸­å¿ƒåŒ–æœ‰å¾ˆå¤šsuper appã€‚ä¸¤æ–¹é¢ï¼šåŸºäºç”¨æˆ·æœ¬åœ°ç¯å¢ƒçš„ä¼˜åŒ–ï¼Œä»å¤´åˆ›é€ æ–°çš„ç¯å¢ƒï¼ˆDeepResearchï¼‰ã€‚

æœ‰ä»·å€¼çš„åˆ›ä¸šæ–¹å‘ï¼šç§¯ç´¯user contextï¼Œæˆ–æ„å»ºç‰¹æ®Šç¯å¢ƒçš„å…¬å¸ï¼Œuser contextå°±åƒå‘æ˜æ±½è½¦ä¹‹å‰çš„çŸ³æ²¹ï¼Œæœ‰æœºä¼šã€‚å¦‚æœintelligenceå¯ä»¥æ™®åŠï¼Œé‚£ä¹ˆåƒè…¾è®¯å¾®ä¿¡è¿™æ ·æ‹¥æœ‰å¹³å°ã€ç¯å¢ƒã€contextä¼šæ˜¯å£å’ã€‚äººç±»ç½‘ç»œä¼šå˜æˆå•¥æ ·ï¼Œå–å†³äºæˆ‘ä»¬æœ‰æ›´å¤šäººç±»æœ‹å‹è¿˜æ˜¯æ›´å¤šagentæœ‹å‹ã€‚

CEOä¸ç§‘å­¦å®¶ï¼šç»„ç»‡æ¶æ„ä¹Ÿåƒé€šç”¨æ–¹æ³•å¯ä»¥åˆ›é€ å¾ˆå¤šä¸œè¥¿ï¼ˆæ¯”å¦‚ç¡…è°·ï¼‰ï¼Œå®ƒå’Œç§‘å­¦å®¶çš„å‘æ˜åˆ›æ–°è·¯å¾„æœ‰åŒºåˆ«ã€‚

é€šç”¨çš„Mindsetï¼šè¿™ä¸ªæ—¶ä»£åšä¸Šé™æ›´é«˜çš„äº‹æƒ…æ›´å¥½ï¼Œæƒ³è±¡åŠ›è¦ä¸°å¯Œï¼Œä»€ä¹ˆéƒ½çˆ±çœ‹ï¼Œæƒ³å˜å¾—é€šç”¨ï¼Œè€Œåˆ›é€ æ¯”äººæ›´é€šç”¨çš„ä¸œè¥¿æ›´æœ‰æ„æ€ã€‚çœ‹ã€Šæ™ºèƒ½ç®€å²ã€‹ã€ä¼ è®°ã€‚

# Summary

ğŸ”­ Summarizing 30 key takeaways from Shunyu Yao's (OpenAI Researcher) latest in-depth 3-hour interview, where he reflects on #agent #research trends, the boundaries between humans and systems, and the future of an #AI world that is simultaneously unified and pluralistic.

ğŸ§  He emphasized that agents must interact with the world, reason, and generalize â€” with language models providing powerful priors that enable this reasoning â€” and that the future of agents lies in both intrinsic-reward exploration and multi-agent organizational systems.

ğŸ’¡ The bottleneck is no longer model training but defining meaningful tasks, environments, and robust reward signals that avoid hacking, while improving context handling, memory, and lifelong learning capabilities.

ğŸ¯ He highlighted opportunities for startups to design new interaction modes, accumulate user context as a moat, and build valuable ecosystems, predicting that future agents will be integrated into larger environments, with language and long-term memory as the key to scaling.

Shunyu Yaoâ€™s perspective is both systematic and forward-looking, with several points worth highlighting:

Language as the Core of Reasoning ğŸ’¬: He frames language-based agents as more fundamental than other agents because language carries prior knowledge â€” enabling reasoning, and thus generalization. LLMs are not just chat tools but universal reasoning engines, making language the â€œinterface of interfacesâ€ for future multimodal agents.

Reward Design > Bigger Models ğŸŒ‰: Reinforcement learningâ€™s real bottleneck is not the algorithm but designing well-defined tasks and white-box, low-noise rewards. Challenge is not to get a bigger model, but to define meaningful, computable goals that drive exploration and avoid reward hacking.

Bridging Technology and Business ğŸŒŒ: He not only lays out the technical roadmap (memory, context, multi-agent, intrinsic motivation) but also stresses that startups must combine new interaction paradigms with model capability overflow, and build data flywheels as moats. He sees agents as a co-evolution of technology and business models, not just a research topic.

Pragmatism with Long-Term Vision ğŸ§©: next few years may still be dominated by centralized super-apps, but also emphasizes the need for lifelong learning, generalization, and distributed systems in the long run â€” giving both near-term execution guidance and a north star for the future.

Encouraging Contrarian Thinking ğŸ§: Calling for â€œhigh-ceilingâ€ projects and contrarian bets, urging researchers and entrepreneurs to focus on imagination, curiosity, and creating new interaction modes â€” rather than just chasing leaderboards.

# References

- [LinkedIn: 30 Takeaways from Shunyu Yao's Talk on Agentic AI](https://www.linkedin.com/pulse/30-takeaways-from-shunyu-yaos-talk-agentic-ai-jf-ai-dqz6c/)
- Full interview Video (in Chinese) å¼ å°çºå¯¹OpenAIå§šé¡ºé›¨3å°æ—¶è®¿è°ˆï¼š6å¹´Agentç ”ç©¶ã€äººä¸ç³»ç»Ÿã€åå™¬çš„è¾¹ç•Œã€æ—¢å•æåˆå¤šå…ƒçš„ä¸–ç•Œ: https://www.youtube.com/watch?v=gQgKkUsx5q0&t=3245s
- Shunyu Yaoâ€™s â€œThe Second Halfâ€: https://ysymyth.github.io/The-Second-Half/

