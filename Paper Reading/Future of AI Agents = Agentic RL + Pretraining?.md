# Future of AI Agents = Agentic RL + Pretraining?

BrowserAgentè¿˜æ˜¯éœ€è¦æœ‰visionèƒ½åŠ›

æµè§ˆå™¨å’Œè™šæ‹Ÿæœºsandboxçš„å››ç§agentï¼š

- 1ã€åŸºäºæµè§ˆå™¨çš„agentï¼›Browserè®¤ä¸ºï¼Œä¸–ç•Œä¸Šæ‰€æœ‰ç½‘ç»œæœåŠ¡éƒ½èƒ½åœ¨æŸä¸ªç½‘é¡µå‘ˆç°ï¼Œåªè¦æˆ‘çš„agentèƒ½çœ‹åˆ°ç½‘é¡µå°±èƒ½å»æ“ä½œï¼Œç”¨æˆ·å¯ä»¥å¯è§†åŒ–çœ‹agentåŠ¨ä½œï¼Œä½†å¾ˆæ…¢ä¸”tokensæ¶ˆè€—å¤§ã€‚
- 2ã€æµè§ˆå™¨+è™šæ‹Ÿæœºçš„agentï¼ˆè™šæ‹Ÿæœºä»£ç +å‘½ä»¤è¡Œï¼‰ï¼›å†™Pythonè„šæœ¬åœ¨è™šæ‹Ÿæœºè·‘ç„¶åå®Œæˆä»»åŠ¡ï¼Œå¯ä»¥è¿è¡Œä»»ä½•çº¿ä¸‹open source packageï¼Œä½†å¾ˆå¤šæƒ…å†µä¸‹ä¸èƒ½è®¿é—®äº’è”ç½‘ï¼ˆOAuthï¼‰ã€‚
- 3ã€è™šæ‹Ÿæœºï¼ˆè™šæ‹Ÿæœºå†…éƒ¨æœ‰å¾ˆå¤§é™åˆ¶çš„agentï¼Œä¸»è¦é€šè¿‡LLMèƒ½åŠ›ç”Ÿæˆåªèƒ½è¿è¡ŒæŸç±»å‹çš„ä»£ç ï¼‰ï¼›å¦‚Gensparkï¼Œåœ¨é™åˆ¶ç¯å¢ƒä¸‹ä»¥LLMä¸ºä¸»ä½“ï¼Œåœ¨sandboxå†™ä»£ç è¿è¡Œåç”Ÿæˆå¯è§†åŒ–ï¼Œç”¨å°ç¯å¢ƒæ‰§è¡Œã€‚ä½†è¿™ä¸ªsandbox is very limitedï¼Œåªæœ‰æœ‰é™çš„ä¸‰å››ä¸ªç¯å¢ƒï¼Œæ— æ³•ä¸´æ—¶ä¸‹è½½packageæˆ–ä¿®æ”¹ï¼Œå°é—­å·¥ä½œæµã€‚
- 4ã€æ¨ªè·¨å¤šå·¥å…·é›†æˆçš„agentã€‚å·¥ä½œæµå†…éƒ¨æ¯ä¸ªèŠ‚ç‚¹å’Œç¬¬ä¸‰æ–¹æœåŠ¡ç›´æ¥é›†æˆï¼Œæ¯ä¸ªæœåŠ¡deliverableå¾ˆå¯é ï¼Œä½†ä¹Ÿä¸èƒ½å…¨åšåˆ°ã€‚

2ã€3çš„è¾¹ç•Œæ¨¡ç³Šï¼ŒåŒºåˆ«åœ¨äº2çš„sandboxæ‰§è¡Œå®Œåæ˜¯å¼€æ”¾sandboxï¼Œsandboxèƒ½åŠ›æ˜¯è¿ä½œä¸»ä½“ï¼›3æ˜¯LLMæ˜¯é™åˆ¶agentçš„ä¸»ä½“ã€‚

- Manusï¼šå°½å¯èƒ½ç”¨ä¸€ä¸ªsandbox + browserç¯å¢ƒæ­å»ºä¸€ä¸ªä¸‡èƒ½çš„åœºæ™¯ï¼ŒLLM planningå®Œåè¿›å…¥browserç”±å¦ä¸€ä¸ªagentå®Œæˆæµè§ˆå™¨ä¿¡æ¯ï¼Œæ€»ç»“ä¿¡æ¯åå†ç»™sandboxæ‰§è¡Œã€‚ï¼ˆä¼¼ä¹Manusçš„sandboxç¯å¢ƒæ¯”ChatGPTå¥½ï¼‰
- ä½†ä¹Ÿè¢«browserèƒ½åŠ›é™åˆ¶ï¼Œæ¯”å¦‚ä¿®æ”¹google sheet cellï¼ˆattentionå¤ªå°ï¼‰ã€åœ¨æŸä¸ªbuttonä¸Šä¼ å›¾ç‰‡ï¼Œè€Œä¸”ä¹Ÿå¾ˆæ…¢ï¼ŒåŠå°æ—¶ä¸€ä¸ªtaskã€‚
- Gensparkï¼šæœ‰superagentåšä»»ä½•äº‹ï¼Œä½†å¤„ç†å·¥å…·æ•°é‡æœ‰é™ï¼Œå¼€å§‹åštemplateï¼ˆsheet, browser, AI call, slideséƒ½æ˜¯agentæ ‡åŒ–å·¥ä½œæµï¼Œåœ¨åŒä¸€ç”¨ä¾‹ä¸‹æŠŠç”¨æˆ·ä½“éªŒä½œä¸ºæ ¸å¿ƒç›®æ ‡ï¼Œå°±è¶Šæ¥è¶Šä¸æ˜¯é€šç”¨agentè€Œæ˜¯äººä¸ºé€‰æ‹©çš„agentï¼Œä½†é€Ÿåº¦ä¹Ÿå¿«å› ä¸ºæ²¡æœ‰å¾ˆå¤§browser navigationä¸”sandboxæœ‰é™ä¸”èŠ‚çœtokenåˆ°ç»†åˆ†åº”ç”¨åœºæ™¯ï¼Œå˜æˆæ‰¿è½½å¾ˆå¤šå°ä»»åŠ¡çš„å¤§å¹³å°å°±åƒå¾®ä¿¡å°ç¨‹åºï¼‰
- Pokeeé€Ÿåº¦æå‡å››å€ï¼šä¸ç”¨å¤æ‚sandboxå’Œtool callingï¼Œç”¨ç¬¬ä¸‰æ–¹é›†æˆçš„SDKå·¥å…·ï¼Œé€šè¿‡Pokeeçš„å·¥å…·è°ƒç”¨å¯ä»¥æé€Ÿï¼ˆæ²¡æœ‰MCPã€tool calling long contextï¼‰å¹¶å‰Šå‡tool callingæˆæœ¬ä¸€åŠï¼Œå†åŠ ä¸Šcontext engineeringã€‚

- æœªæ¥å·¥ä½œæµï¼šä¸å†æ˜¯browserå„ä¸ªç½‘é¡µæ“ä½œï¼Œè€Œæ˜¯ecommerce/æœç´¢/è§†é¢‘ç½‘ç«™çš„é—¨æˆ·æµé‡ä¸‹é™ï¼Œå…¥å£å˜æˆå„æ–¹å‘agentï¼Œæœ‰A2Aäº¤äº’ï¼ˆAgentå…¥å£ï¼‰ã€‚ç°åœ¨MCPå¯ç”¨æ€§å·®ï¼Œå¸‚é¢ä¸Šåªæœ‰200/20000ä¸ªMCPå¥½ç”¨ä¸”éš¾ç»´æŠ¤ï¼Œç›®æ ‡æ˜¯å…¬å¸ä¸å†éœ€è¦åšMCPï¼Œç›´æ¥æä¾›SDKç„¶åå°±è·å¾—é¢å¤–æµé‡ã€‚
- åˆ›ä½œè€…/SaaSç”Ÿæ€çš„å•†ä¸šæ¨¡å¼ï¼šAgentæ–¹æ¯æ¬¡è°ƒç”¨ï¼Œéœ€è¦å‘çŸ¥è¯†äº§æƒå¹³å°æ–¹ä»˜æ¬¾ï¼Œå¹¿å‘Šç”±agentå®Œæˆï¼Œç”¨æˆ·é€‰æ‹©ä¸åŒagentï¼Œç”±rankingæœºåˆ¶æ¨èå“ªä¸ªagentå»åšï¼Œå°±å‘é‚£ä¸ªagentå…¬å¸æ”¶é’±ï¼Œå¹¿å‘Šåœ¨è¿™æ—¶å‘ç”Ÿã€‚è€ŒçŸ¥è¯†äº§æƒæœ¬èº«å¯ä»¥ç›´æ¥æ”¶è´¹ï¼Œå› æ­¤åˆ›ä½œè€…/SaaSç”Ÿæ€å˜å¥½äº†ï¼Œä¸å†éœ€è¦è°·æ­ŒæŠ•å¹¿å‘Šï¼Œè€Œæ˜¯agentç›´æ¥å‘ä½ ä»˜è´¹ã€‚
- ranking-basedæ¨èç³»ç»Ÿæ–¹å‘å¯èƒ½å—åˆ°å·¨å¤§æŒ¤å‹ï¼Œåœ¨agentæ¡†æ¶ä¸‹ï¼Œæ¨èç³»ç»Ÿä»æ˜¯ç«¯åˆ°ç«¯multi-round decision makingï¼Œä½†æ¯æ¬¡äº¤äº’åªæ˜¯å°‘é‡ä¿¡æ¯ç»“æœï¼Œå†³ç­–çº¿ä¸å†æ˜¯rankingè€Œæ˜¯æ—¶é—´ï¼Œå› ä¸ºäººå’Œagentäº¤äº’æ—¶é•¿æœ‰é™ï¼Œagentç›®æ ‡æ˜¯æ¨èä¸œè¥¿è®©ä½ èŠ±è´¹æ—¶é—´å’Œå®ƒçš„å›æŠ¥æˆæ­£æ¯”ï¼Œè¿™æ ·æ¨èç³»ç»Ÿç®—æ³•å°±ä¸æˆç«‹ï¼ˆåŸæ¥ï¼šç‚¹å‡»ç‡ä¸rankingæˆæ­£æ¯”ï¼›ç°åœ¨ï¼šæ¨èçš„ä¿¡æ¯æ˜¯ä½ å¿…ç‚¹çš„ä¸œè¥¿ä½†æœ‰ç¬¬äºŒè½®äº¤äº’ï¼Œä¸‹ä¸€æ¬¡äº¤äº’æ—¶é—´å†…æ¨èçš„ä¸œè¥¿æ˜¯æœ€ç²¾å‡†çš„ä»è€Œæœ‰æ›´å¤šäº¤äº’ï¼Œå› æ­¤æ˜¯10æ¡conversationä¸”æ¯ä¸€è½®ç›®æ ‡éƒ½æ˜¯è®©ä½ åšä¸‹ä¸€è½®äº¤äº’ï¼Œä¸å†æ˜¯rankè€Œæ˜¯sequentialåŸºäºä½“éªŒå’Œæ¢ç´¢çš„äº¤äº’æœºåˆ¶ï¼Œåœ¨ä¸æŸå¤±æœªæ¥opportunity costæƒ…å†µä¸‹åŒç­‰çº§åˆ«contentå†…é€‰æ‹©æœ€å¤šæ”¶å…¥çš„å†…å®¹ï¼‰ã€‚


- å¼ºåŒ–å­¦ä¹ è®©agentå†³ç­–å¯èƒ½ä¸å†æ˜¯token-basedï¼ŒDeep Researchæ˜¯token by tokenï¼ŒAgentic Systemå¯èƒ½å·¥å…·æ˜¯tokenizedä¸œè¥¿ï¼Œå¤šä¸ªå·¥å…·ä¸€èµ·è§£å†³é—®é¢˜ï¼Œæ˜¯ç›®æ ‡é©±åŠ¨ã€‚è€ŒLLMå¯ä»¥supervised learning autoregressiveè®­ç»ƒï¼Œä½†agentic systemå¾ˆéš¾ï¼š
- å•ä¸€tool callingå¯ä»¥é€šè¿‡æ•°æ®å®Œæˆï¼Œä½†å˜æˆå·¥å…·é“¾çš„æ—¶å€™ï¼Œå¾ˆéš¾å®Œæˆautoregressive trainingï¼Œå› ä¸ºå¸‚é¢ä¸Šä¸å­˜åœ¨æ•°æ®è€Œåªèƒ½äººä¸ºæ ‡æ³¨

RL pretrainingè€ŒéRLHFï¼šå› ä¸ºæœ‰å¾ˆå¤šä»»åŠ¡æ˜¯åªæœ‰ç›®æ ‡é©±åŠ¨çš„ï¼Œå¤§å¤šæ•°æ˜¯æ²¡æœ‰æ•°æ®çš„ï¼Œè€Œä¸”é—®é¢˜éå¸¸æ³›åŒ–ï¼Œéœ€è¦ç”Ÿæˆå°ä¼—çš„é¢†åŸŸæ•°æ®è¾“å‡ºï¼Œé€šè¿‡ground-truth validatorå‘Šè¯‰å¯¹ä¸å¯¹ç„¶åself-trainï¼Œè¿™ç§è®­ç»ƒé€‚ç”¨äºæœ‰ground truthä¸”èƒ½ç²¾å‡†åˆ¤æ–­çš„ç”¨ä¾‹å¹¶ä¼˜åŒ–ï¼Œé€‚åˆRLã€‚

AGIï¼ˆäº§ç”ŸçŸ¥è¯†æ˜¯äººç±»ä¸æ‹¥æœ‰çš„ï¼‰ï¼šå¦‚ä½•æå‡verificationæœºåˆ¶æ³›åŒ–æ€§ï¼ŸAgentè¾“å‡ºåç¦»äººç±»åå¥½æ—¶ï¼Œå¦‚ä½•è®©verifier adaptåˆ°æ–°è¾“å‡ºä¸Šä½¿å…¶å®Œæˆæ›´å¥½verifyï¼Ÿç¬¬ä¸€ï¼Œå¦‚æœèƒ½é€šè¿‡äººç±»æè¿°ï¼Œä»é¢†åŸŸAæ¨ç†å‘é¢†åŸŸBçš„verificationæ˜¯ä»€ä¹ˆï¼Œé‚£ä¹ˆagent verificationæ³›åŒ–æ€§æé«˜ã€‚ç¬¬äºŒï¼Œèƒ½å¦è‡ªæˆ‘æ¢ç´¢ï¼ŒåŸºäºç°æœ‰çŸ¥è¯†groundingï¼Œå®Œæˆå¯¹æœªæ¥çŸ¥è¯†verificationçš„å»¶ä¼¸ã€‚
agentä¹‹é—´äº’ç›¸å†²çªæ‰€äº§ç”Ÿçš„çŸ›ç›¾ï¼Œä¼šå¼•å‘æ”¿æ²»å—ï¼Ÿ

å½“å‰verificationæ³›åŒ–è¿›å±•ä¸å¤§ï¼Œæ˜¯äººç±»çŸ¥è¯†è’¸é¦çš„æ³›åŒ–æå‡ã€‚RLæ˜¯counter-factual learningï¼Œå¯èƒ½å‡ºç°èƒ½è§£å†³é—®é¢˜ä½†äººç±»çœ‹ä¸æ‡‚çš„æ–¹æ¡ˆï¼Œè¿™ä¸ªå¼±ç‚¹å¯¼è‡´reward definitionå¾ˆé‡è¦ï¼Œè®­ç»ƒç»™ä»€ä¹ˆincentiveå†³å®šäº†è®­ç»ƒå‡ºçš„agentæ˜¯ä»€ä¹ˆæ ·ã€‚

- æ•°æ®æ ‡æ³¨åœ¨multi-modalityï¼ˆè§†é¢‘ã€å›¾ç‰‡ï¼‰æ˜¯é€ƒä¸å¼€çš„ï¼Œå› ä¸ºverificationåŸºäºè§†é¢‘å’Œå›¾ç‰‡çš„Reinforce fine tuningï¼Œimage inputè§£æèƒ½åŠ›è¦æ±‚é«˜ä¸”ä¸èƒ½é human ruleå®Œæˆï¼Œå¿…é¡»é æ¨¡å‹è§£æèƒ½åŠ›æŠŠå›¾ç‰‡å’Œè§†é¢‘å†…å®¹è§£æï¼Œåœ¨æ­¤ä¹‹ä¸Šäººç±»æ‰èƒ½å†™ruleã€‚ä½†è§£æå›¾ç‰‡ç»†èŠ‚çš„èƒ½åŠ›å¾ˆéš¾ã€‚
- Reinforce fine tuningï¼šä¸è¦reward modelï¼Œå°±ç”¨å¤§å®¶å…±è¯†æˆ–LLM-as-a-judgeè®­ç»ƒæ¨¡å‹ã€‚
- Multi-modalityï¼šå¤§é‡æ•°æ®è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œç„¶åRL fine tuningï¼Œç„¶åæ€æ ·åšæ ‡å‡†åŒ–çš„judge/rule-based verifierï¼Œç›®å‰ä¸å­˜åœ¨ï¼Œå› ä¸ºå›¾åƒæœ¬èº«æ²¡æœ‰æ ‡å‡†ç­”æ¡ˆã€‚ç›®å‰æ˜¯å…ˆé€šè¿‡æ•°æ®è®­ç»ƒreward modelä½¿å¾—multi-modalityèƒ½åŠ›æœ€å¤§ï¼Œç„¶åé€šè¿‡è¾“å…¥è¾“å‡ºèƒ½åŠ›æŠŠå®ƒå˜æˆverifierç„¶åå†reinforce fine tuningã€‚

Alignmentæ˜¯æŠ€æœ¯é—®é¢˜ï¼Œroboticsçš„è¡Œä¸ºå¾ˆéš¾æœ‰rubricsï¼Œå› æ­¤multi-modal+actionsçš„æ•°æ®æ ‡æ³¨å¾ˆéš¾ã€‚

AI Agentæœªæ¥å½¢æ€ï¼šå¸Œæœ›æ˜¯åƒä¸€è¡Œpromptè°ƒç”¨APIæ¥å£ä¸€æ ·ä½¿ç”¨ChatGPTä¸€æ ·ç®€å•ï¼Œè€Œä¸ç”¨æ‹…å¿ƒbrowserç¯å¢ƒ+infraå»bypass agentå·¥å…·çš„èƒ½åŠ›æ¥å‹ç¼©å·¥å…·æ•°é‡ã€‚browseræ˜¯ä¸€ä¸ªå·¥å…·ä»£æ›¿å‡ åƒä¸ªå·¥å…·ï¼Œæˆ‘ä»¬å¸Œæœ›æ¨¡å‹èƒ½åŠ›æœ€å¼ºã€å·¥å…·æŠŠå®ƒé“ºå¼€ï¼Œè®©agentå»æ“çºµæ•´ä¸ªäº’è”ç½‘ï¼Œä¸€ä¸ªagentå®Œæˆå·¥å…·é€‰æ‹©ã€è§„åˆ’ã€ç»“æœï¼ˆè€Œä¸éœ€è¦infraä¹‹é—´æ¥å›è·³è½¬ï¼Œè€Œåªéœ€è¦promptè¾“å…¥APIå°±å¯ä»¥äº§ç”Ÿç»“æœï¼‰ã€‚

å‚ç›´é€‰ä¸€ç³»åˆ—å·¥å…·çš„æ¨ç†æ¨¡å‹ï¼šè¦ç›´æ¥accesså·¥å…·å¾—åˆ°ç²¾ç¡®æ•°æ®å†åˆ†æçš„workflowï¼Œæ­£å¸¸browseræä¸å®šå› ä¸ºåœ¨è®­ç»ƒä¸­ä»æœªè§è¿‡æ•°æ®ï¼Œéœ€è¦foundation modelå¼€å‘æ¥æ‹“å±•workflowç±»å‹ï¼Œè€Œä¸åªé™å®šäºè´­ç‰©å’Œç ”ç©¶ï¼Œå¾ˆå¤šä¸“ä¸šæ€§workflowè¿˜è§£å†³ä¸äº†ã€‚

Model plasticityå¯å¡‘æ€§ï¼šæ¨¡å‹è®­ç»ƒåˆ°ä¸€å®šç¨‹åº¦å°±ä¼šç¾éš¾æ€§é—å¿˜ï¼Œéœ€è¦continual learningè§£å†³ã€‚

<img width="1000" height="1500" alt="image" src="https://github.com/user-attachments/assets/955f6435-a3a2-491c-b86f-df2d7ff3ac34" />


# Future of AI Agents = Agentic RL + Pretraining?

As AI rapidly evolves, weâ€™re witnessing a shift in how agents are designed, trained, and deployed. The discussion around BrowserAgents, sandboxed environments, and reinforcement learning (RL) is heating up â€” and for good reason. Here are my key takeaways and reflections.

## ğŸŒ Four Types of Browser + VM Agents
- Pure Browser Agents. View the web as the â€œworldâ€ â€” if it can be rendered in a browser, the agent can act on it. Pros: Fully visualizable; users can watch every action. Cons: Extremely slow, token-heavy, expensive.
- Browser + VM Hybrid Agents. Combines browser automation with code execution inside a virtual machine. Can run Python scripts, use offline open-source packages, but often blocked from internet access (e.g., OAuth issues).
- Sandbox-First Agents. Example: Genspark. The agent generates code inside a tightly restricted sandbox environment and runs it. Limitation: Can only use a fixed set of 3â€“4 environments, cannot dynamically download packages or modify the environment.
- Multi-Tool Integrated Agents. Workflow-based approach: each node is a direct integration with a service. Very reliable deliverables, but limited flexibility â€” not a truly general-purpose agent.

Key distinction between (2) and (3): 
- (2) = sandbox is the executor (open environment).
- (3) = LLM is the executor (sandbox is restrictive).

## ğŸ—ï¸ Emerging Architectures: Manus, Genspark, Pokee
- Manus: Attempts to unify sandbox + browser into one universal agent framework. LLM does planning â†’ passes plan to browser agent â†’ collects info â†’ executes in sandbox. Limitation: still bottlenecked by browser capabilities (e.g., editing Google Sheets, image upload).
- Genspark: â€œSuperagentâ€ approach. Standardized workflows (Sheets, Browser, AI Call, Slides) to improve UX. Faster because it avoids heavy browser navigation and token costs â€” optimized for targeted use cases.
- Future: Speed-optimized platform. Skips complex sandbox/tool-calling overhead by using third-party SDKs. Achieves 4Ã— speedup and halves tool-calling costs via context engineering.

## ğŸ”® Future of Workflows: From Browser Ops to Agent-to-Agent (A2A)
- We may move away from browser-based navigation toward agents as the new entry points for e-commerce, search, and content discovery.
- MCP ecosystem challenge: today, only ~200 of 20,000 MCPs are reliable and maintainable.
- Vision: companies simply provide SDKs â†’ agents gain native access â†’ ecosystem grows.

## Business Model Implications
- Agents directly pay SaaS/creators for IP usage per call.
- Ads shift from Google-driven to agent-driven (ranking and selection).
- Recommendation systems transform: No longer just ranking for clicks. Shift to sequential decision-making â€” maximizing meaningful user-agent interactions within limited attention windows.

## ğŸ§  RL Pretraining (not just RLHF): Toward AGI
- Problem: Agentic systems canâ€™t be trained purely with supervised autoregressive learning â€” multi-step toolchains lack training data.
- Solution: RL Pretraining (not just RLHF). Works best for goal-driven, data-scarce tasks where ground truth validators can tell â€œright vs wrong.â€ Enables self-training on synthetic data for long-horizon, generalized reasoning.

Most real-world tasks are goal-driven, not data-rich. They often lack demonstrations entirely, especially in niche or specialized domains. Instead of RLHF (which aligns a model with human preferences), we can use RL pretraining:

- Generate domain-specific outputs with a ground-truth validator.
- Self-train by reinforcing correct solutions and penalizing incorrect ones.
- Optimize for generalization in cases where no large dataset exists.

This is especially powerful when we have precise ways to determine correctness.

## âœ… Hard Problem: Verification & Alignment Challenges
- Key Research Question: How do we generalize verifiers to handle novel outputs and unseen domains? Can we reason from domain A to domain Bâ€™s verification logic using human-provided descriptions?
- Risks: RL may yield solutions humans canâ€™t interpret (counterfactual learning). Reward definition becomes mission-critical â€” the agent you get is the agent you incentivize.
- Multi-Modality Barriers: Verification on images/videos requires strong perception models. Reinforcement Fine-Tuning (RFT) may replace reward models by using consensus signals or LLM-as-a-judge. But image/video verification still lacks standardized judge/rule-based frameworks.

Today, verification mechanisms are mostly distilled from human knowledge. The big open questions:

- How do we generalize verification? If an agent outputs something outside known human preferences, how do we adapt our verifier to evaluate it? (1) Can we reason across domains â€” infer what verification in Domain B should look like from Domain Aâ€™s rules? (2) Can verifiers self-explore and extrapolate based on existing grounding to handle future knowledge?

Agent disagreement is another open frontier â€” will conflicting agents create a form of â€œpoliticsâ€?

## Multi-Modal RL Fine-Tuning: Why Data is Inevitable
When verification involves images or video, we cannot rely solely on hand-crafted human rules. We need models capable of:

- Parsing visual input accurately.
- Producing structured representations humans can reason about.
- Allowing rules or LLM-as-a-judge systems to operate on top of these representations.

Today, we train large multi-modal models first, then use reward modeling + RL fine-tuning to maximize perception capabilities. But thereâ€™s still no widely adopted standard for judge/rule-based verification in vision tasks â€” this is a key research gap.

## The Future of AI Agents
The ideal AI agent should feel as simple as sending a single API call â€” no complex browser environments, no juggling infrastructure just to make tools work.

- Browser as a meta-tool: replaces thousands of individual APIs, but we still need better workflow coverage.
- Workflow plasticity: foundation models must support direct tool access for precise data retrieval, analysis, and planning â€” not just shopping and research, but professional workflows too.
- ğŸ§© Rich Sutton's Model Plasticity & Continual learning: avoid catastrophic forgetting as agents adapt to new domains without losing prior skills..

Ultimately, we want goal-driven agents that can plan, act, and verify autonomously â€” with RL-pretraining making them robust in the absence of large-scale training data.

## ğŸš€ My Take
The future agent will feel as simple as calling an API â€” no more worrying about browser hacks or tool orchestration. Building AGI isnâ€™t just about better LLMs â€” itâ€™s about combining multi-modal understanding, RL-pretrained decision-making, and scalable verification systems that can keep up with agents generating knowledge humans donâ€™t yet have. We are heading toward a world where:

- One agent can autonomously plan, select tools, execute tasks, and deliver results.
- RL pretraining becomes the norm for long-horizon decision-making.
- Verification generalization + multi-modal reasoning will be key unlocks for AGI.

This is the turning point where reinforcement learning becomes the backbone of intelligent, goal-driven agents.

## References

- [LinkedIn: Future of AI Agents = Agentic RL + Pretraining?](https://www.linkedin.com/pulse/future-ai-agents-agentic-rl-pretraining-jf-ai-p0nlc/)

- [ç¡…è°·101æ’­å®¢E201 feat. æœ±å“²æ¸…ï½œä¸‹ä¸€ä¸ªAIå‰æ²¿æ–¹å‘ï¼šå¼ºåŒ–å­¦ä¹ é¢„è®­ç»ƒä¸AGIçš„è½¬ç‚¹æ—¶åˆ»ï¼ˆä¸Šã€ä¸‹ï¼‰](https://www.youtube.com/watch?v=3Kygpc3rCCo)(https://www.youtube.com/watch?v=52MNTycuS1o)
